---
title: Exploring Boardgames with R, XGBoost with scikit-learn
description: Using R to do some basic EDA of this board games dataset and then using scikit-learn to see what makes a game successful
author: Duncan
date: '2022-02-05'
slug: boardgames-exploration-and-modeling
categories: [modeling, xgboost, python, r]
tags: [modeling, python, r]
editor_options: 
  chunk_output_type: console
---

Improving scikit-learn skills and seeing how reticulate interacts with blogdown
<!--more-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      error = FALSE,
                      comment = "#>",
                      collapse = TRUE)

library(tidyverse)
library(hrbrthemes)
library(gt)
library(cowplot)
library(reticulate)
theme_set(theme_ipsum_rc())

ratings <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv')
details <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv')
```

When the pandemic first started, we begun seeing quite a few adverts for board games on our social media which was probably linked to the fact that we kept searching for things to do! One that stood out to us was Veiled Fate, which became my other half's first board game purchase. Two years later and the game still hasn't shipped, however, our love for board games has only grown since then and we've purchased a lot of games we wouldn't have thought of before. A particular favourite of mine is Root! Anyway, that's why when this week's [TidyTuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-01-25/readme.md) came out with a great dataset from [BGG](https://boardgamegeek.com/), I knew I had to get involved.

At work my colleagues only use Python so as someone who loves R it's been a bit of a learning experience switching over. That said, the `reticulate` package (and Databricks' ability to use R/Python/SQL in one notebook) is great for allowing me to do EDA in a language I'm comfortable in, and modeling in a language people around the business will understand. So here I'll begin having a look at the dataset using regular old R and then switch to Python's `scikit-learn` to see if my board game idea would be a hit with BGG browsers.

There are two separate dataframes: `ratings` which contains the name of the game, year it was released, rank, average score, and amount of reviews. Then there's `details` which gives us a lot of indepth detail about the actual game, such as min/max players, how long it takes to play, and what kind of game it is. We can join the two using the `id` column.

```{r}
ratings_joint <-
  ratings %>%
  left_join(details, by = "id")

ratings_joint %>%
  select(rank, name, year, average, users_rated, minplayers, maxplayers, boardgamecategory) %>%
  arrange(rank) %>%
  head(10) %>%
  gt() %>%
  opt_all_caps() %>%
  opt_table_font(
    font = list(
      google_font("Roboto"),
      default_fonts()
    )
  ) %>% 
  tab_options(
    column_labels.background.color = "white",
    table.border.top.width = px(3),
    table.border.top.color = "white",
    table.border.bottom.color = "white",
    table.border.bottom.width = px(3),
    column_labels.border.top.width = px(3),
    column_labels.border.top.color = "white",
    column_labels.border.bottom.width = px(3),
    column_labels.border.bottom.color = "black",
    data_row.padding = px(3),
    table.font.size = 12,
    heading.align = "right"
  ) 
```

The top 10 are dominated by games released in the last 10 or so years, so I wonder if there's a general trend in newer games being received better as publishers learn what people like. In terms of categories, for the model we'll have to split them out but we can quickly see that people love economic games and adventure also ranks highly. It appears that 4 max players is the sweet spot, and people also love playing alone! I've never tried Gloomhaven but have watched quite a few playthroughs and from what I know, it's a highly complex game so it's interesting to see how well received it is. 


```{r echo=FALSE, message=FALSE}
p1 <-
  ratings_joint %>% 
  ggplot(aes(average)) +
  geom_histogram() +
  labs(x = "Average score",
       y = "Ratings",
       title = "Distribution of average score") +
  theme(plot.title.position = "plot",
        plot.title = element_text(size = 10),
        plot.subtitle = element_text(size = 9),
        axis.text.x = element_text(size = 6),
        axis.text.y = element_text(size = 6),
        legend.title = element_blank())

p2 <-
  ratings_joint %>%
  filter(yearpublished > 1950,
         yearpublished < 2022) %>%
  ggplot(aes(yearpublished)) +
  geom_bar() +
  labs(x = "Year",
       y = "Game releases",
       title = "Number of games released by year") +
  theme(plot.title.position = "plot",
        plot.title = element_text(size = 10),
        plot.subtitle = element_text(size = 9),
        axis.text.x = element_text(size = 6),
        axis.text.y = element_text(size = 6),
        legend.title = element_blank())

plot_grid(p1, p2)
```

We can see quickly that the amount of board games releasing has gone up substantially. That said, it seems to have slowed down during 2020, potentially due to the pandemic and global shortages.

```{r echo=FALSE}
ratings_joint %>%
  filter(yearpublished > 1950,
         yearpublished < 2022) %>%
  group_by(yearpublished) %>%
  summarise(average = mean(average, na.rm=T)) %>%
  ggplot(aes(yearpublished, average)) +
  geom_line()
```

At the same time, average rating has gone up for board games. There could be a few reasons for this but seems like publishers are aware of what does well with audiences. It'll be interesting to see what the model picks out as key variables for scoring highly - maybe more games are incorporating economic aspects, or maybe games have just gotten better as the years passed.

# Quick data prep

There are a lot of features in the original data but to keep the model simple, I'm going to discard most of the 'boardgame' adjacent variables like family, expansion etc.. I will however keep mechanic and category as I think these can help predict what kind of game will be successful. I'll also get rid of any NAs (there aren't many), and clean up the category and mechanic columns.

```{r}
ratings_joint_d <-
  ratings_joint %>%
  select(average, contains("min"), contains("max"), boardgamecategory, boardgamemechanic) %>%
  mutate(boardgamecategory = str_to_lower(boardgamecategory),
         boardgamecategory = str_squish(boardgamecategory),
         boardgamemechanic = str_to_lower(boardgamemechanic),
         boardgamemechanic = str_squish(boardgamemechanic)) %>% 
  drop_na() %>%
  rename("mechanic_" = boardgamemechanic,
         "category_" = boardgamecategory)
```


XGBoost is one of the most popular models these days because it can handle both regression and classification. As I'm dealing with a continuous target variable in *average*, I'll be using the regression function. To get this to work I need to split out the categories and mechanics, and then turn these into dummy variables so the model knows how to interpret each feature. I would usually use a fold and test what parameters to use but because I'm just trying to see how reticulate interacts with blogdown I'm going to stick to a simple training/testing set.

```{python include=FALSE}
import pandas as pd

ratings_joint_py = r.ratings_joint_d
ratings_joint_py
```

```{python}
#chars = "[]'\""

onehot = ['mechanic_', 'category_']

for cat in onehot:
    ratings_joint_py.loc[:,cat] = ratings_joint_py[cat].str.replace('[^A-Za-z0-9_,]+', "", regex=True).copy()
    onehotencoded = ratings_joint_py[cat].str.get_dummies(sep=',',).add_prefix(cat)
    onehotencoded.fillna(0, inplace=True)
    print(cat, len(onehotencoded.columns))
    ratings_joint_py = pd.concat([ratings_joint_py.drop(cat, axis=1), onehotencoded], axis=1)
    ratings_joint_py = ratings_joint_py.loc[:,~ratings_joint_py.columns.duplicated()].copy()
    
print("{rows} rows and {cols} columns".format(rows = len(ratings_joint_py), cols = len(ratings_joint_py.columns)))
```


# Model

```{python echo=FALSE}
import xgboost as xgb
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as MSE

X, y = ratings_joint_py.iloc[:,1:], ratings_joint_py.iloc[:,0]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
```


```{python}
xgb_regressor = xgb.XGBRegressor(objective = 'reg:linear', n_estimators = 10, seed = 345)
xgb_regressor.fit(X_train, y_train)
pred = xgb_regressor.predict(X_test)

rmse = np.sqrt(MSE(y_test, pred))
print("RMSE: % f" %(rmse))
```

Not the greatest score of all time but considering we didn't test any specific parameters, that is pretty good. I want to quickly see what features were important in predicting the ratings for this model.

# Model visualisation

Thanks to the `xgboost` package I can use a simply command to visualise these features:

```{python}
import matplotlib.pyplot as plt

xgb.plot_importance(xgb_regressor, max_num_features = 20)
plt.rcParams['figure.figsize'] = [6,4]
plt.rcParams.update({'font.size':6})
plt.tight_layout()
plt.show()
```

However, if I want to create a graph in R I'll just have to do some extra steps and create a new pandas dataframe but it's nice to see how you can simply use the two languages one after another:

**Python syntax**:

```{python}
feature_importance = xgb_regressor.get_booster().get_score(importance_type='weight')
keys = list(feature_importance.keys())
values = list(feature_importance.values())

feature_importance_df = pd.DataFrame(data=values, index=keys, columns=["score"]).sort_values(by = "score", ascending=False)
```

**R syntax**: 

```{r}
py$feature_importance_df %>%
  head(20) %>%
  rownames_to_column(var = "feature") %>%
  ggplot(aes(fct_reorder(feature, score), score)) +
  geom_point() +
  coord_flip() +
  theme(plot.title.position = "plot",
        plot.title = element_text(size = 10),
        plot.subtitle = element_text(size = 9),
        axis.text.x = element_text(size = 6),
        axis.text.y = element_text(size = 6),
        legend.title = element_blank()) +
  labs(x = NULL, y = NULL,
         title = "Feature importance")
```

Done! It seems very easy to switch between the two languages. I especially like how `reticulate` makes it possible to call dataframes between the two languages without any issue. I did have some problems with installing Python packages but... even when not using RStudio that happens so can only blame the end-user.
