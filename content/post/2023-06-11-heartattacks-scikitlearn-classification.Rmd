---
title: Using scikit's pipeline for a Heart Attack classification model
description: Using a mix of R and Python for EDA and modelling
draft: true
author: Duncan
date: '2023-06-11'
slug: heart-attack-classification-pipeline
categories: [modeling, classification, python, r]
tags: [modeling, python, r]
editor_options: 
  chunk_output_type: console
---

Testing a pipeline for variety of classification models
<!--more-->

1. Check data for nulls / types
2. Check imbalance of target variable
3. Check correlation between variables
4. Split dataset into train/test
5. Try model evaluation with no changes
6. 
. Build pipeline (try StandardScaler)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      error = FALSE,
                      comment = "#>",
                      collapse = TRUE)

library(tidyverse)
library(hrbrthemes)
library(reticulate)
library(here)
theme_set(theme_ipsum_rc())

heart_df <- readr::read_csv(here("content", "post", "heart.csv"))
```

```{r echo=FALSE, message=FALSE}
heart_df %>% head(5)
```

The columns are as follows:

> * Age (age in years)
* Sex (1 = male; 0 = female)
* CP (chest pain type)
* TRESTBPS (resting blood pressure (in mm Hg on admission to the hospital))
* CHOL (serum cholestoral in mg/dl)
* FPS (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)
* RESTECH (resting electrocardiographic results)
* THALACH (maximum heart rate achieved)
* EXANG (exercise induced angina (1 = yes; 0 = no))
* OLDPEAK (ST depression induced by exercise relative to rest)
* SLOPE (the slope of the peak exercise ST segment)
* CA (number of major vessels (0-3) colored by flourosopy)
* THAL (3 = normal; 6 = fixed defect; 7 = reversable defect)
* TARGET (1 or 0)

Let's have a look and see if there are null values and if anything weird jumps out with the distributions:

```{r echo=FALSE, message=FALSE}
heart_df %>% summary()
```

Thankfully with it being a Kaggle dataset, a lot of the cleaning etc. has already been done and there are no null values here. Additionally, the dataset has been updated so variables like sex and chest pain type are already numerically encoded which means we don't have to do that. We'll use a boxplot to see if there are outliers that jump out:

```{r}
heart_df %>%
  select(age, trestbps, chol, thalach) %>%
  reshape2::melt() %>%
  ggplot() +
  geom_boxplot(aes(x = variable, y = value, colour = variable)) +
  facet_wrap(~variable, scales="free") +
  labs(title = "Spotting outliers",
       x = NULL,
       y = NULL) +
  theme(legend.position = "none")
```

We can see that `age` is fine but `trestbps`, `chol`, and `thalach` all have outliers that we need to be aware of. Not going to do anything to them yet but will see if changing those makes a difference to the model evaluation. Lastly I'll check if there are obvious correlations between variables and if there's a class imbalance for the target variable, and then on to the model!

```{r}
corrplot::corrplot(cor(heart_df, method="spearman"))
```

```{r echo=FALSE}
heart_df %>%
  select(target) %>%
  group_by(target) %>%
  count() %>%
  ungroup() %>%
  mutate(n_pct = n / sum(n)) %>%
  ggplot(aes(n_pct)) +
  geom_bar() +
  labs(title = "Class balance for target",
       y = NULL,
       x = NULL) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme(axis.text.x = element_blank())
```

All good here :)

# Modelling

Preferably we would've tried things like one hot encoding, filling null values etc. all in a single pipeline but because the dataset we're working with is already great, we don't need to do that. So instead, we'll just do the scaling in the pipeline so we can easily switch between 3 models. For this, I'll use Python

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier


heart_df_p = r.heart_df
heart_df_p.head()
```

And then create a training/test split:

```{python}
X = heart_df_p.drop('target', axis=1)
y = heart_df_p['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)
```

```{python}
heart_df_p['target'].shape
```


