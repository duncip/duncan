[
    
        
            {
                "ref": "https://duncip.netlify.app/2021/06/11/pythontest.html",
                "title": "pythonTest",
                "section": "post",
                "date" : "2021.06.11",
                "body": "\r\rHere we go. Let’s create a ggplot first and then we’ll try the same with matplotlib\nkey_crops %\u0026gt;%\rgroup_by(country) %\u0026gt;%\rsummarise(avg = mean(wheat_tonnes_per_hectare, na.rm = TRUE)) %\u0026gt;%\rarrange(desc(avg)) %\u0026gt;%\rhead(10) %\u0026gt;%\rggplot(aes(country, avg)) +\rgeom_col()\rNow try the same in Python..\nimport pandas as pd\rimport matplotlib.pyplot as plt\rp_df = pd.read_csv(\u0026#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/key_crop_yields.csv\u0026#39;)\rp_df.head()\r## Entity ... Bananas (tonnes per hectare)\r## 0 Afghanistan ... NaN\r## 1 Afghanistan ... NaN\r## 2 Afghanistan ... NaN\r## 3 Afghanistan ... NaN\r## 4 Afghanistan ... NaN\r## ## [5 rows x 14 columns]\rWe’ve loaded the DF\nt = p_df.groupby(\u0026quot;Entity\u0026quot;).agg(averageVal = (\u0026quot;Wheat (tonnes per hectare)\u0026quot;, \u0026quot;mean\u0026quot;)).sort_values(\u0026quot;averageVal\u0026quot;, ascending=False).head(10)\rt.reset_index(inplace=True)\rplt.bar(t[\u0026quot;Entity\u0026quot;], t[\u0026quot;averageVal\u0026quot;])\r## \u0026lt;BarContainer object of 10 artists\u0026gt;\rplt.show()\rdwdw\n"
            }
        
    ,
        
            {
                "ref": "https://duncip.netlify.app/2020/11/08/crops-europe-and-gt.html",
                "title": "Crops, Europe, and {gt}",
                "section": "post",
                "date" : "2020.11.08",
                "body": "\nThe gt package in particular, which was released by the amazing RStudio team, seems to have been getting lots of attention. There have been several TidyTuesday entries where people have used tables with the most notable (to me, from memory) entries by Thomas Mock who has written a few tutorials. Based on those tutorials I wanted to have a look myself.\nTables are a necessity where I work to represent big piles of web data that can’t be visualised easily and one of the issues we run into is that everyone uses different formats. Even the same person, based on what week they’re doing something, will produce something completely different. gt seems to address this with a theme function so you can have cool consistent tables like the BBC or FiveThirtyEight, so I’ll be trying that.\nFor the data I wanted to do something simple: look at global crop yields and focus on European countries to see which is the top exporter for each crop throughout the decades. The below has several chunks stripped out, full code can be found on GitHub!\nkey_crops %\u0026gt;%\rdistinct(country) %\u0026gt;% nrow()\r## [1] 249\rScraping and readying data\rWith 249 countries there’s a lot to process. Instead, I just want to focus on Europe. Because I don’t want to put together a list manually, I’ll use the Wiki page to get a quick list and also to extract the flag icons for the table!\nurl \u0026lt;- \u0026quot;https://en.wikipedia.org/wiki/List_of_European_countries_by_population\u0026quot;\rwiki_scrape \u0026lt;- url %\u0026gt;%\rread_html() %\u0026gt;%\rhtml_nodes(xpath = \u0026#39;//*[@id=\u0026quot;mw-content-text\u0026quot;]/div[1]/table\u0026#39;) wiki_table \u0026lt;-\rwiki_scrape %\u0026gt;%\rhtml_table(fill = TRUE)\rnames(wiki_table[[1]]) \u0026lt;- c(\u0026quot;rank\u0026quot;, \u0026quot;country\u0026quot;, \u0026quot;population\u0026quot;, \u0026quot;estimate\u0026quot;, \u0026quot;date\u0026quot;, \u0026quot;region\u0026quot;, \u0026quot;source\u0026quot;)\rwiki_table[[1]]$country \u0026lt;- gsub(\u0026#39;\\\\[.*?\\\\]\u0026#39;, \u0026#39;\u0026#39;, wiki_table[[1]]$country)\reu_list \u0026lt;- as_tibble(wiki_table[[1]])[-1,]\rhead(eu_list, 4)\r## # A tibble: 4 x 7\r## rank country population estimate date region source ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 Russia 146,052,135 146,877,088 1 Jan 20~ \u0026quot;EAEU\u0026quot; National Estimate[6] ## 2 2 Turkey 84,867,581 83,154,997 4 Feb 20~ \u0026quot;\u0026quot; National estimate[7] ## 3 3 Germany 84,030,234 82,887,000 30 Jun 2~ \u0026quot;EU\u0026quot; National estimate[8] ## 4 4 France 65,311,156 67,076,000 1 Mar 20~ \u0026quot;EU\u0026quot; Monthly national estim~\rNow that we have the list, I’ll extract the flags separately. I also need to fix the URL as gt requires you to put the full url rather than the relative one:\nflags \u0026lt;- wiki_scrape %\u0026gt;%\rhtml_nodes(\u0026quot;img\u0026quot;) %\u0026gt;%\rhtml_attr(\u0026quot;src\u0026quot;) %\u0026gt;%\ras_tibble() %\u0026gt;%\rrename(\u0026quot;flag_url\u0026quot; = value) %\u0026gt;%\rmutate(country = str_extract(flag_url, \u0026quot;(?\u0026lt;=of_).+(?=.svg/)\u0026quot;),\rcountry = case_when(\rstr_detect(country, \u0026quot;Kingdom\u0026quot;) ~ \u0026quot;United Kingdom\u0026quot;,\rstr_detect(country, \u0026quot;Netherlands\u0026quot;) ~ \u0026quot;Netherlands\u0026quot;,\rstr_detect(country, \u0026quot;Belgium\u0026quot;) ~ \u0026quot;Belgium\u0026quot;,\rTRUE ~ country\r),\rflag_url = str_replace(flag_url, \u0026quot;\\\\/\\\\/\u0026quot;, \u0026quot;https://\u0026quot;))\rhead(flags, 4)\r## # A tibble: 4 x 2\r## flag_url country\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 https://upload.wikimedia.org/wikipedia/en/thumb/f/f3/Flag_of_Russia.s~ Russia ## 2 https://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Flag_of_Tur~ Turkey ## 3 https://upload.wikimedia.org/wikipedia/en/thumb/b/ba/Flag_of_Germany.~ Germany\r## 4 https://upload.wikimedia.org/wikipedia/en/thumb/c/c3/Flag_of_France.s~ France\rWith the list of European countries and flag URLs sorted, I had a quick look through the data and found that no countries in Europe produce cocoa beans or cassava so I’ve filtered both these out. Now we can have a quick look at recurring countries.\n\rTable time!\rIn terms of the data, I want to use percentages rather than actual yield so the numbers are comparable. I’ll also simply slice the top for each crop, every decade, so we have the most productive countries only. This is the data I’ll use for the first table:\ntop_eu_table_data \u0026lt;-\reu_crop_data %\u0026gt;%\rgroup_by(decade, crop) %\u0026gt;%\rmutate(decade_crop_pct = ifelse(yield == 0, 0, yield / sum(yield))) %\u0026gt;%\rungroup() %\u0026gt;%\rgroup_by(decade, crop) %\u0026gt;%\rslice_max(decade_crop_pct, n = 1) %\u0026gt;%\rungroup() %\u0026gt;%\rleft_join(flags, by = \u0026quot;country\u0026quot;) %\u0026gt;%\rselect(-country)\rTo show you what the table would look like with any alterations at all, here’s the first few lines..\nhtml {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#yeewiwzfab .gt_table {\rdisplay: table;\rborder-collapse: collapse;\rmargin-left: auto;\rmargin-right: auto;\rcolor: #333333;\rfont-size: 16px;\rfont-weight: normal;\rfont-style: normal;\rbackground-color: #FFFFFF;\rwidth: auto;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #A8A8A8;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #A8A8A8;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\r}\r#yeewiwzfab .gt_heading {\rbackground-color: #FFFFFF;\rtext-align: center;\rborder-bottom-color: #FFFFFF;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#yeewiwzfab .gt_title {\rcolor: #333333;\rfont-size: 125%;\rfont-weight: initial;\rpadding-top: 4px;\rpadding-bottom: 4px;\rborder-bottom-color: #FFFFFF;\rborder-bottom-width: 0;\r}\r#yeewiwzfab .gt_subtitle {\rcolor: #333333;\rfont-size: 85%;\rfont-weight: initial;\rpadding-top: 0;\rpadding-bottom: 4px;\rborder-top-color: #FFFFFF;\rborder-top-width: 0;\r}\r#yeewiwzfab .gt_bottom_border {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#yeewiwzfab .gt_col_headings {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#yeewiwzfab .gt_col_heading {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\rpadding-left: 5px;\rpadding-right: 5px;\roverflow-x: hidden;\r}\r#yeewiwzfab .gt_column_spanner_outer {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rpadding-top: 0;\rpadding-bottom: 0;\rpadding-left: 4px;\rpadding-right: 4px;\r}\r#yeewiwzfab .gt_column_spanner_outer:first-child {\rpadding-left: 0;\r}\r#yeewiwzfab .gt_column_spanner_outer:last-child {\rpadding-right: 0;\r}\r#yeewiwzfab .gt_column_spanner {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\roverflow-x: hidden;\rdisplay: inline-block;\rwidth: 100%;\r}\r#yeewiwzfab .gt_group_heading {\rpadding: 8px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\r}\r#yeewiwzfab .gt_empty_group_heading {\rpadding: 0.5px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: middle;\r}\r#yeewiwzfab .gt_from_md  :first-child {\rmargin-top: 0;\r}\r#yeewiwzfab .gt_from_md  :last-child {\rmargin-bottom: 0;\r}\r#yeewiwzfab .gt_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rmargin: 10px;\rborder-top-style: solid;\rborder-top-width: 1px;\rborder-top-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\roverflow-x: hidden;\r}\r#yeewiwzfab .gt_stub {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-right-style: solid;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rpadding-left: 12px;\r}\r#yeewiwzfab .gt_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#yeewiwzfab .gt_first_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\r}\r#yeewiwzfab .gt_grand_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#yeewiwzfab .gt_first_grand_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: double;\rborder-top-width: 6px;\rborder-top-color: #D3D3D3;\r}\r#yeewiwzfab .gt_striped {\rbackground-color: rgba(128, 128, 128, 0.05);\r}\r#yeewiwzfab .gt_table_body {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#yeewiwzfab .gt_footnotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#yeewiwzfab .gt_footnote {\rmargin: 0px;\rfont-size: 90%;\rpadding: 4px;\r}\r#yeewiwzfab .gt_sourcenotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#yeewiwzfab .gt_sourcenote {\rfont-size: 90%;\rpadding: 4px;\r}\r#yeewiwzfab .gt_left {\rtext-align: left;\r}\r#yeewiwzfab .gt_center {\rtext-align: center;\r}\r#yeewiwzfab .gt_right {\rtext-align: right;\rfont-variant-numeric: tabular-nums;\r}\r#yeewiwzfab .gt_font_normal {\rfont-weight: normal;\r}\r#yeewiwzfab .gt_font_bold {\rfont-weight: bold;\r}\r#yeewiwzfab .gt_font_italic {\rfont-style: italic;\r}\r#yeewiwzfab .gt_super {\rfont-size: 65%;\r}\r#yeewiwzfab .gt_footnote_marks {\rfont-style: italic;\rfont-size: 65%;\r}\r\r\rcode\rdecade\rcrop\ryield\rdecade_crop_pct\rflag_url\r\r\r\rESP\r1960\rbananas\r36.213167\r0.41607086\rhttps://upload.wikimedia.org/wikipedia/en/thumb/9/9a/Flag_of_Spain.svg/23px-Flag_of_Spain.svg.png\r\r\rNLD\r1960\rbarley\r3.906722\r0.07424106\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r\r\rNLD\r1960\rbeans\r2.272400\r0.12436583\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r\r\r\r The way I defined the function is by altering the table one line at a time. I then decided what I could reuse for future tables and put it in a function. For example, I started with the above table and slowly added things like opt_all_caps() and got it to look how I wanted. Once that was done, I cut and pasted the reusable code into a function. My final function looks like this:\ngt_theme_cropc \u0026lt;- function(data,...) {\rdata %\u0026gt;%\ropt_all_caps() %\u0026gt;%\ropt_table_font(\rfont = list(\rgoogle_font(\u0026quot;Roboto Condensed\u0026quot;),\rdefault_fonts()\r)\r) %\u0026gt;%\rtab_style(\rstyle = cell_borders(\rsides = \u0026quot;bottom\u0026quot;, color = \u0026quot;transparent\u0026quot;, weight = px(2)\r),\rlocations = cells_body(\rcolumns = TRUE,\rrows = nrow(data$`_data`)\r)\r) %\u0026gt;% text_transform( locations = cells_body(\rcontains(\u0026quot;flag\u0026quot;)\r),\rfn = function(x) {\rweb_image(\rurl = x,\rheight = 10\r)\r}\r) %\u0026gt;%\rtab_style(\rstyle = list(\rcell_text(weight = \u0026quot;bold\u0026quot;)\r), locations = cells_body(\rcolumns = vars(crop)\r)\r) %\u0026gt;%\rcols_label(\rcrop = \u0026quot;Crop\u0026quot;\r) %\u0026gt;%\rtab_source_note(\rsource_note = md(\u0026quot;**DATA**: [TidyTuesday wk 36](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-09-01/readme.md) \u0026lt;br\u0026gt;**TABLE**: Duncan P\u0026quot;)\r) %\u0026gt;%\rtab_options(\rcolumn_labels.background.color = \u0026quot;white\u0026quot;,\rtable.border.top.width = px(3),\rtable.border.top.color = \u0026quot;transparent\u0026quot;,\rtable.border.bottom.color = \u0026quot;transparent\u0026quot;,\rtable.border.bottom.width = px(3),\rcolumn_labels.border.top.width = px(3),\rcolumn_labels.border.top.color = \u0026quot;transparent\u0026quot;,\rcolumn_labels.border.bottom.width = px(3),\rcolumn_labels.border.bottom.color = \u0026quot;black\u0026quot;,\rdata_row.padding = px(3),\rsource_notes.font.size = 8,\rtable.font.size = 12,\rheading.align = \u0026quot;left\u0026quot;,\rtable.align = \u0026quot;left\u0026quot;,\r...\r) }\rLet’s see what the table looks like\rtop_eu_data_tab \u0026lt;- top_eu_table_data %\u0026gt;%\rselect(code, decade, decade_crop_pct, flag_url, crop) %\u0026gt;%\rrename(\u0026quot;country\u0026quot; = code) %\u0026gt;% pivot_wider(names_from = decade, values_from = c(country, flag_url, decade_crop_pct)) %\u0026gt;% select(crop, flag_url_1960, contains(\u0026quot;1960\u0026quot;), flag_url_1970, contains(\u0026quot;1970\u0026quot;), flag_url_1980, contains(\u0026quot;1980\u0026quot;), flag_url_1990, contains(\u0026quot;1990\u0026quot;), flag_url_2000, contains(\u0026quot;2000\u0026quot;), flag_url_2010, contains(\u0026quot;2010\u0026quot;)) %\u0026gt;%\rgt() %\u0026gt;%\rfmt_percent(\rcolumns = contains(\u0026quot;pct\u0026quot;),\rdecimals = 0\r) %\u0026gt;%\rtab_header(\rtitle = md(\u0026quot;**Top exporter by crop for European countries through the decades**\u0026quot;),\rsubtitle = md(\u0026quot;% represents proportion of **total crop** export that decade\u0026quot;)\r) %\u0026gt;%\rcols_label(\rflag_url_1960 = \u0026quot;\u0026quot;,\rflag_url_1970 = \u0026quot;\u0026quot;,\rflag_url_1980 = \u0026quot;\u0026quot;,\rflag_url_1990 = \u0026quot;\u0026quot;,\rflag_url_2000 = \u0026quot;\u0026quot;,\rflag_url_2010 = \u0026quot;\u0026quot;,\rdecade_crop_pct_1960 = \u0026quot;\u0026quot;,\rdecade_crop_pct_1970 = \u0026quot;\u0026quot;,\rdecade_crop_pct_1980 = \u0026quot;\u0026quot;,\rdecade_crop_pct_1990 = \u0026quot;\u0026quot;,\rdecade_crop_pct_2000 = \u0026quot;\u0026quot;,\rdecade_crop_pct_2010 = \u0026quot;\u0026quot;,\rcountry_1960 = \u0026quot;60s\u0026quot;,\rcountry_1970 = \u0026quot;70s\u0026quot;,\rcountry_1980 = \u0026quot;80s\u0026quot;,\rcountry_1990 = \u0026quot;90s\u0026quot;,\rcountry_2000 = \u0026quot;2000s\u0026quot;,\rcountry_2010 = \u0026quot;2010s\u0026quot;\r) %\u0026gt;%\rgt_theme_cropc()\rgtsave(top_eu_data_tab, \u0026quot;top_eu_data_tab.png\u0026quot;) # save as img as web_image doesn\u0026#39;t show on mobile\rI like it! There are a few issues, mainly with differing flag sizes because of the way they were uploaded on Wikipedia. We could potentially manually alter these by creating a column in the flags dataframe and using the web_image() function to transform images based on height / width specified in that but for the sake of this exercise, I’m happy with this. The main thing I’m unhappy with is the column names as I want to do something similar for my second table but unfortunately cols_label() does not yet seem to support match() regex functions.\nJust to demonstrate how I could now make a similar looking table with the function, I thought it’d be interesting to look at Netherlands and the UK in isolation and see where they each rank for every crop. Will remove bananas, rice, and soybeans as neither country export these. This is the data I’ll use:\nuk_nl_ranked \u0026lt;- eu_crop_data %\u0026gt;%\rgroup_by(decade, crop) %\u0026gt;%\rmutate(decade_crop_pct = ifelse(yield == 0, 0, yield / sum(yield))) %\u0026gt;%\rarrange(desc(decade_crop_pct), .by_group = TRUE) %\u0026gt;% mutate(rank = row_number()) %\u0026gt;% ungroup() %\u0026gt;%\rfilter(code %in% c(\u0026quot;GBR\u0026quot;, \u0026quot;NLD\u0026quot;)) %\u0026gt;% mutate(rank = ifelse(yield == 0, 0, rank)) %\u0026gt;%\rleft_join(flags, by = \u0026quot;country\u0026quot;) %\u0026gt;%\rselect(code, decade, rank, flag_url, crop) %\u0026gt;%\rrename(\u0026quot;country\u0026quot; = code) %\u0026gt;%\rfilter(!crop %in% c(\u0026quot;bananas\u0026quot;, \u0026quot;rice\u0026quot;, \u0026quot;soybeans\u0026quot;)) %\u0026gt;%\rpivot_wider(names_from = c(decade, country), values_from = c(rank, flag_url)) %\u0026gt;% select(crop, matches(\u0026quot;flag.*60.*NLD\u0026quot;), matches(\u0026quot;60_NLD\u0026quot;), matches(\u0026quot;flag.*60.*GBR\u0026quot;), matches(\u0026quot;60_GBR\u0026quot;),\rmatches(\u0026quot;flag.*70.*NLD\u0026quot;), matches(\u0026quot;70_NLD\u0026quot;), matches(\u0026quot;flag.*70.*GBR\u0026quot;), matches(\u0026quot;70_GBR\u0026quot;),\rmatches(\u0026quot;flag.*80.*NLD\u0026quot;), matches(\u0026quot;80_NLD\u0026quot;), matches(\u0026quot;flag.*80.*GBR\u0026quot;), matches(\u0026quot;80_GBR\u0026quot;), matches(\u0026quot;flag.*90.*NLD\u0026quot;), matches(\u0026quot;90_NLD\u0026quot;), matches(\u0026quot;flag.*90.*GBR\u0026quot;), matches(\u0026quot;90_GBR\u0026quot;), matches(\u0026quot;flag.*2000.*NLD\u0026quot;), matches(\u0026quot;2000_NLD\u0026quot;), matches(\u0026quot;flag.*2000.*GBR\u0026quot;), matches(\u0026quot;2000_GBR\u0026quot;), matches(\u0026quot;flag.*2010.*NLD\u0026quot;), matches(\u0026quot;2010_NLD\u0026quot;), matches(\u0026quot;flag.*2010.*GBR\u0026quot;), matches(\u0026quot;2010_GBR\u0026quot;))\rUnaltered table\rhtml {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#lsbtrjdhci .gt_table {\rdisplay: table;\rborder-collapse: collapse;\rmargin-left: auto;\rmargin-right: auto;\rcolor: #333333;\rfont-size: 16px;\rfont-weight: normal;\rfont-style: normal;\rbackground-color: #FFFFFF;\rwidth: auto;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #A8A8A8;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #A8A8A8;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\r}\r#lsbtrjdhci .gt_heading {\rbackground-color: #FFFFFF;\rtext-align: center;\rborder-bottom-color: #FFFFFF;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#lsbtrjdhci .gt_title {\rcolor: #333333;\rfont-size: 125%;\rfont-weight: initial;\rpadding-top: 4px;\rpadding-bottom: 4px;\rborder-bottom-color: #FFFFFF;\rborder-bottom-width: 0;\r}\r#lsbtrjdhci .gt_subtitle {\rcolor: #333333;\rfont-size: 85%;\rfont-weight: initial;\rpadding-top: 0;\rpadding-bottom: 4px;\rborder-top-color: #FFFFFF;\rborder-top-width: 0;\r}\r#lsbtrjdhci .gt_bottom_border {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#lsbtrjdhci .gt_col_headings {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#lsbtrjdhci .gt_col_heading {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\rpadding-left: 5px;\rpadding-right: 5px;\roverflow-x: hidden;\r}\r#lsbtrjdhci .gt_column_spanner_outer {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rpadding-top: 0;\rpadding-bottom: 0;\rpadding-left: 4px;\rpadding-right: 4px;\r}\r#lsbtrjdhci .gt_column_spanner_outer:first-child {\rpadding-left: 0;\r}\r#lsbtrjdhci .gt_column_spanner_outer:last-child {\rpadding-right: 0;\r}\r#lsbtrjdhci .gt_column_spanner {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\roverflow-x: hidden;\rdisplay: inline-block;\rwidth: 100%;\r}\r#lsbtrjdhci .gt_group_heading {\rpadding: 8px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\r}\r#lsbtrjdhci .gt_empty_group_heading {\rpadding: 0.5px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: middle;\r}\r#lsbtrjdhci .gt_from_md  :first-child {\rmargin-top: 0;\r}\r#lsbtrjdhci .gt_from_md  :last-child {\rmargin-bottom: 0;\r}\r#lsbtrjdhci .gt_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rmargin: 10px;\rborder-top-style: solid;\rborder-top-width: 1px;\rborder-top-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\roverflow-x: hidden;\r}\r#lsbtrjdhci .gt_stub {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-right-style: solid;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rpadding-left: 12px;\r}\r#lsbtrjdhci .gt_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#lsbtrjdhci .gt_first_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\r}\r#lsbtrjdhci .gt_grand_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#lsbtrjdhci .gt_first_grand_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: double;\rborder-top-width: 6px;\rborder-top-color: #D3D3D3;\r}\r#lsbtrjdhci .gt_striped {\rbackground-color: rgba(128, 128, 128, 0.05);\r}\r#lsbtrjdhci .gt_table_body {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#lsbtrjdhci .gt_footnotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#lsbtrjdhci .gt_footnote {\rmargin: 0px;\rfont-size: 90%;\rpadding: 4px;\r}\r#lsbtrjdhci .gt_sourcenotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#lsbtrjdhci .gt_sourcenote {\rfont-size: 90%;\rpadding: 4px;\r}\r#lsbtrjdhci .gt_left {\rtext-align: left;\r}\r#lsbtrjdhci .gt_center {\rtext-align: center;\r}\r#lsbtrjdhci .gt_right {\rtext-align: right;\rfont-variant-numeric: tabular-nums;\r}\r#lsbtrjdhci .gt_font_normal {\rfont-weight: normal;\r}\r#lsbtrjdhci .gt_font_bold {\rfont-weight: bold;\r}\r#lsbtrjdhci .gt_font_italic {\rfont-style: italic;\r}\r#lsbtrjdhci .gt_super {\rfont-size: 65%;\r}\r#lsbtrjdhci .gt_footnote_marks {\rfont-style: italic;\rfont-size: 65%;\r}\r\r\rcrop\rflag_url_1960_NLD\rrank_1960_NLD\rflag_url_1960_GBR\rrank_1960_GBR\rflag_url_1970_NLD\rrank_1970_NLD\rflag_url_1970_GBR\rrank_1970_GBR\rflag_url_1980_NLD\rrank_1980_NLD\rflag_url_1980_GBR\rrank_1980_GBR\rflag_url_1990_NLD\rrank_1990_NLD\rflag_url_1990_GBR\rrank_1990_GBR\rflag_url_2000_NLD\rrank_2000_NLD\rflag_url_2000_GBR\rrank_2000_GBR\rflag_url_2010_NLD\rrank_2010_NLD\rflag_url_2010_GBR\rrank_2010_GBR\r\r\r\rbarley\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r1\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r3\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r1\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r4\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r2\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r4\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r3\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r5\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r5\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r7\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r3\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r7\r\r\rbeans\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r1\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r0\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r2\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r0\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r3\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r0\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r1\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r0\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r3\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r0\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r4\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r0\r\r\rmaize\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r3\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r0\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r5\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r12\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r5\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r16\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r5\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r0\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r3\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r0\rhttps://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\r1\rhttps://upload.wikimedia.org/wikipedia/en/thumb/a/ae/Flag_of_the_United_Kingdom.svg/23px-Flag_of_the_United_Kingdom.svg.png\r0\r\r\r\r\rTable with function\ruk_nl_ranked_tab \u0026lt;- uk_nl_ranked %\u0026gt;%\rgt() %\u0026gt;%\rcols_label(\rflag_url_1960_GBR = \u0026quot;\u0026quot;,\rflag_url_1970_GBR = \u0026quot;\u0026quot;,\rflag_url_1980_GBR = \u0026quot;\u0026quot;,\rflag_url_1990_GBR = \u0026quot;\u0026quot;,\rflag_url_2000_GBR = \u0026quot;\u0026quot;,\rflag_url_2010_GBR = \u0026quot;\u0026quot;,\rflag_url_1960_NLD = \u0026quot;\u0026quot;,\rflag_url_1970_NLD = \u0026quot;\u0026quot;,\rflag_url_1980_NLD = \u0026quot;\u0026quot;,\rflag_url_1990_NLD = \u0026quot;\u0026quot;,\rflag_url_2000_NLD = \u0026quot;\u0026quot;,\rflag_url_2010_NLD = \u0026quot;\u0026quot;,\rrank_1960_GBR = \u0026quot;\u0026quot;,\rrank_1970_GBR = \u0026quot;\u0026quot;,\rrank_1980_GBR = \u0026quot;\u0026quot;,\rrank_1990_GBR = \u0026quot;\u0026quot;,\rrank_2000_GBR = \u0026quot;\u0026quot;,\rrank_2010_GBR = \u0026quot;\u0026quot;,\rrank_1960_NLD = \u0026quot;\u0026quot;,\rrank_1970_NLD = \u0026quot;\u0026quot;,\rrank_1980_NLD = \u0026quot;\u0026quot;,\rrank_1990_NLD = \u0026quot;\u0026quot;,\rrank_2000_NLD = \u0026quot;\u0026quot;,\rrank_2010_NLD = \u0026quot;\u0026quot;\r) %\u0026gt;%\rtab_spanner(\rlabel = \u0026quot;60s\u0026quot;,\rcolumns = 2:5\r) %\u0026gt;%\rtab_spanner(\rlabel = \u0026quot;70s\u0026quot;,\rcolumns = 6:9\r) %\u0026gt;%\rtab_spanner(\rlabel = \u0026quot;80s\u0026quot;,\rcolumns = 10:13\r) %\u0026gt;%\rtab_spanner(\rlabel = \u0026quot;90s\u0026quot;,\rcolumns = 14:17\r) %\u0026gt;%\rtab_spanner(\rlabel = \u0026quot;2000s\u0026quot;,\rcolumns = 18:21\r) %\u0026gt;%\rtab_spanner(\rlabel = \u0026quot;2010s\u0026quot;,\rcolumns = 22:25\r) %\u0026gt;%\rtab_header(\rtitle = md(\u0026quot;**The Netherlands and the UK ranked by crop yield ratios**\u0026quot;),\rsubtitle = md(\u0026quot;0 means *unranked*\u0026quot;)\r) %\u0026gt;%\rgt_theme_cropc()\rgtsave(uk_nl_ranked_tab, \u0026quot;uk_nl_ranked_tab.png\u0026quot;) # save as img as web_image doesn\u0026#39;t show on mobile\rIt’s definitely not the easiest table to read but can see that it’s quick to make tables look similar with a simple function. If it weren’t for the drastically different column names, could have included that in the function and reduce the code length drastically. Hopefully in the future gt will allow us to use a match() regex for column names in which case we could reduce the cols_label() parameters to about 3 lines.\nReally enjoyed using gt and look forward to seeing the tables people have submitted for the RStudio data contest. Definitely going to be implementing some themes at work where we can include things like the brand colours and logo!\n\r\r\r"
            }
        
    ,
        
            {
                "ref": "https://duncip.netlify.app/2020/06/27/caribou-and-their-movement.html",
                "title": "Caribou and their movement - maps in R",
                "section": "post",
                "date" : "2020.06.27",
                "body": "\nThis week’s TidyTuesday data is about Caribou and the tracking of their whereabouts. When I first saw the repo entry I got excited as I thought there’d be a great opportunity to do some classification modeling like can we determine sex based on distance traveled? How do the patterns in their traveling differ as they grow older? Unfortunately, most of the data within individuals are incomplete as that didn’t seem to be the aim of the study.\nWhat we have instead is the opportunity of creating some maps in R. I don’t work with maps in my everyday job as we don’t tend to use location data but thought it would be interesting to try, especially as the last few TidyTuesdays have been good for map plots. Upon reading some articles like this great r-spatial article, I thought I’d try two approaches: one with a simple geom_point (after I already checked the data) and one using a drawn map of British Columbia (which thankfully already has a package).\nindividuals \u0026lt;- readr::read_csv(\u0026#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-06-23/individuals.csv\u0026#39;)\rlocations \u0026lt;- readr::read_csv(\u0026#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-06-23/locations.csv\u0026#39;)\rExplore\rI already looked through the individuals dataset which is when I realised the things I wanted to do weren’t feasible. A quick skimr shows that data completeness is very low and even though we perhaps could do something with the deploy_off_type column, the amount of work in trying to get it working is not worth it.\nskimr::skim(individuals)\rSo instead let’s forget about that dataset and instead turn to locations!\nnrow(locations)\r## [1] 249450\rThat is a lot of rows. I don’t think that’ll be a problem for the sf map as it will handle it well but trying to plot 250k points may be an issue. I just want to check if study_site and season are complete:\nprint(str_c(\u0026#39;study_site: \u0026#39;, sum(is.na(locations$study_site)), \u0026#39;, \u0026#39;, \u0026#39;season: \u0026#39;, sum(is.na(locations$season))))\r## [1] \u0026quot;study_site: 0, season: 0\u0026quot;\rAll good. As I’m not plotting on a map in my first graph, I don’t care that much for exact accuracy of the location (within reason) so I’m going to round latitude and longitude to 2 decimals. This should reduce a lot of the ‘duplicates’ and make it easier to plot.\nlocations_deduped \u0026lt;- locations %\u0026gt;%\rmutate(longitude = round(longitude, 2),\rlatitude = round(latitude, 2)) %\u0026gt;%\rdistinct(season, study_site, longitude, latitude)\rThat reduces our data to 18k rows which should be more manageable.\nPoint plot\rNow for the plotting part which, to be honest, is very simple as the amount of data draws what looks like a map for us already.\nggplot(locations_deduped) +\rgeom_point(aes(longitude, latitude, col = study_site)) +\rtheme(legend.position = \u0026#39;bottom\u0026#39;)\rThis is a decemt start. One thing I do immediately notice is that Caribou deploying from the Hart Ranges site seem to be all over the place. Strange as proportionally, there aren’t that many entries from that site. Anyway, all I want to do is:\n\rSplit by season so we can observe the differences\n\rReduce size of points\n\rApply a theme (I read that theme_void() is a great option for maps)\n\rStyle until I’m happy with it!\n\r\rggplot(locations_deduped) +\rgeom_point(aes(longitude, latitude, col = study_site), size = 0.3, alpha = 0.9) +\rgghighlight(unhighlighted_params = list(colour = \u0026quot;#F2EFC7\u0026quot;), use_direct_label = FALSE) +\rpalettetown::scale_colour_poke(pokemon = \u0026quot;golbat\u0026quot;) +\rguides(colour = guide_legend(title = \u0026quot;Study Site\u0026quot;, override.aes = list(size = 4))) +\rfacet_wrap(~season, strip.position = \u0026quot;bottom\u0026quot;) +\rlabs(\rtitle = \u0026quot;\\nSeasonal differences in British Columbian Caribou movement\u0026quot;,\rsubtitle = \u0026quot;Patterns show that during summer, the different herds are much closer together than during winter.\\n Without knowledge of the animal, I assume this is because winter comes with food challenges.\\n\u0026quot;\r) +\rtheme_void() +\rtheme(\r#plot.background = element_rect(fill = \u0026quot;#D2BBA0\u0026quot;),\rplot.background = element_rect(fill = \u0026#39;#BAD9B5\u0026#39;),\rplot.title = element_text(hjust = 0.1, family = \u0026#39;opensans\u0026#39;, colour = \u0026#39;#442B48\u0026#39;, face = \u0026#39;bold\u0026#39;, size = 16),\rplot.subtitle = element_text(hjust = 0.1, family = \u0026#39;opensans\u0026#39;, colour = \u0026#39;#442B48\u0026#39;, size = 10),\r#legend.position = c(0.4, 0.8),\rlegend.position = c(0.8, 0.7),\rlegend.title = element_text(size = 8, family = \u0026#39;roboto\u0026#39;, colour = \u0026#39;#442B48\u0026#39;, face = \u0026#39;bold\u0026#39;),\rlegend.text = element_text(size = 8, family = \u0026#39;roboto\u0026#39;, colour = \u0026#39;#442B48\u0026#39;),\rstrip.background = element_blank(),\rstrip.placement = \u0026quot;outside\u0026quot;,\rstrip.text = element_text(size = 12, family = \u0026#39;opensans\u0026#39;, colour = \u0026#39;#726E60\u0026#39;, face = \u0026#39;bold\u0026#39;)\r)\rgeom_point map faceted by season showing difference in Caribou locations between the seasons.\n\rYou can also enlarge the image to see the detail clearer\ngghighlight is an amazing package that comes in handy when wanting to quickly highlight certain data that needs to stand out (and I use it daily) but unhighlighted_params is a relatively new feature and not used very much yet. It’s very useful and helps us draw the points that don’t have a study_site appointed so we end up with the same map on both sides, even if the data points are different.\nDon’t have much to say about the data as I don’t know a lot about Caribou (or British Columbia) but I find it interesting that the Caribou take up so much more real estate during the summer whereas during the winter there are big stretches of land that are left unattended. I really like the plot though and I think if there is no necessity to draw a map, geom_point seems like the perfect approach.\n\rMap plot\rI also wanted to try a map plot which I’m only able to do because of the amazing bcgov team that put together the package. As part of the package, there’s a function transform_bc_albers() that will help us transform an sf object to a BC Albers projection. Of course, to do that, need to get the dataset into an sf object first!\nlibrary(sf)\rsf_locations \u0026lt;- locations %\u0026gt;%\rselect(study_site, season, latitude, longitude) %\u0026gt;%\rst_as_sf(coords = c(\u0026quot;longitude\u0026quot;, \u0026quot;latitude\u0026quot;)) %\u0026gt;%\rst_set_crs(4326)\rFor me, the most difficult part to wrap my head around was st_set_crs(). All of the examples I saw (and I read many articles!) simply used st_set_crs(4326) but I couldn’t find out what the 4326 was referring to. Turns out it was very simple.. just not for someone who never uses maps! It refers to the World Geodetic System\ngeom_sf map faceted by season showing difference in Caribou locations between the seasons\n\rAgain, a bigger version here\nChanges are much less obvious between the two seasons in this plot but it’s nice to have the backdrop of BC. Makes you realise how little of the space they actually occupy. Can still see that they are more likely to ‘clump’ during summer time! I’m not going to change any more to the graph but I think to make it better I would just forego the facet altogether.\nThis was another great TidyTuesday dataset. I definitely learnt a lot about how to use geom_sf and I liked the playing around with the styles for the geom_point() plot. Bit disappointed I couldn’t do the classification model and maps aren’t really my thing but had put off doing any of the recent TT’s due to their focus on location data (although there were some historically signicant datasets in there that people need to check out).\n\r\r"
            }
        
    ,
        
            {
                "ref": "https://duncip.netlify.app/2020/06/08/exploring-tidymodels-with-regression.html",
                "title": "Exploring Tidymodels with regression",
                "section": "post",
                "date" : "2020.06.08",
                "body": "\nMost of the things below are learnt through materials provided by Julia Silge (her course) and Alison Hill’s course at rstudio::conf(2020). It’s mainly to have a reference for myself in terms of what the Tidymodels workflow is as it makes modeling much easier. I’m not aiming for accuracy of models here (and the comparisons are not great when using RMSE considering different processes), instead, I just want to document the Tidymodels process so I can easily implement it in the future.\nExplore\rLet’s have a look at the data - I don’t want to do too much data manipulation here as want to focus on the aspect of a simple Tidymodels workflow!\names %\u0026gt;%\rhead(5)\r#\u0026gt; # A tibble: 5 x 81\r#\u0026gt; MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape\r#\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 One_Story_~ Resident~ 141 31770 Pave No_A~ Slightly~\r#\u0026gt; 2 One_Story_~ Resident~ 80 11622 Pave No_A~ Regular #\u0026gt; 3 One_Story_~ Resident~ 81 14267 Pave No_A~ Slightly~\r#\u0026gt; 4 One_Story_~ Resident~ 93 11160 Pave No_A~ Regular #\u0026gt; 5 Two_Story_~ Resident~ 74 13830 Pave No_A~ Slightly~\r#\u0026gt; # ... with 74 more variables: Land_Contour \u0026lt;fct\u0026gt;, Utilities \u0026lt;fct\u0026gt;,\r#\u0026gt; # Lot_Config \u0026lt;fct\u0026gt;, Land_Slope \u0026lt;fct\u0026gt;, Neighborhood \u0026lt;fct\u0026gt;, Condition_1 \u0026lt;fct\u0026gt;,\r#\u0026gt; # Condition_2 \u0026lt;fct\u0026gt;, Bldg_Type \u0026lt;fct\u0026gt;, House_Style \u0026lt;fct\u0026gt;, Overall_Qual \u0026lt;fct\u0026gt;,\r#\u0026gt; # Overall_Cond \u0026lt;fct\u0026gt;, Year_Built \u0026lt;int\u0026gt;, Year_Remod_Add \u0026lt;int\u0026gt;,\r#\u0026gt; # Roof_Style \u0026lt;fct\u0026gt;, Roof_Matl \u0026lt;fct\u0026gt;, Exterior_1st \u0026lt;fct\u0026gt;, Exterior_2nd \u0026lt;fct\u0026gt;,\r#\u0026gt; # Mas_Vnr_Type \u0026lt;fct\u0026gt;, Mas_Vnr_Area \u0026lt;dbl\u0026gt;, Exter_Qual \u0026lt;fct\u0026gt;, Exter_Cond \u0026lt;fct\u0026gt;,\r#\u0026gt; # Foundation \u0026lt;fct\u0026gt;, Bsmt_Qual \u0026lt;fct\u0026gt;, Bsmt_Cond \u0026lt;fct\u0026gt;, Bsmt_Exposure \u0026lt;fct\u0026gt;,\r#\u0026gt; # BsmtFin_Type_1 \u0026lt;fct\u0026gt;, BsmtFin_SF_1 \u0026lt;dbl\u0026gt;, BsmtFin_Type_2 \u0026lt;fct\u0026gt;,\r#\u0026gt; # BsmtFin_SF_2 \u0026lt;dbl\u0026gt;, Bsmt_Unf_SF \u0026lt;dbl\u0026gt;, Total_Bsmt_SF \u0026lt;dbl\u0026gt;, Heating \u0026lt;fct\u0026gt;,\r#\u0026gt; # Heating_QC \u0026lt;fct\u0026gt;, Central_Air \u0026lt;fct\u0026gt;, Electrical \u0026lt;fct\u0026gt;, First_Flr_SF \u0026lt;int\u0026gt;,\r#\u0026gt; # Second_Flr_SF \u0026lt;int\u0026gt;, Low_Qual_Fin_SF \u0026lt;int\u0026gt;, Gr_Liv_Area \u0026lt;int\u0026gt;,\r#\u0026gt; # Bsmt_Full_Bath \u0026lt;dbl\u0026gt;, Bsmt_Half_Bath \u0026lt;dbl\u0026gt;, Full_Bath \u0026lt;int\u0026gt;,\r#\u0026gt; # Half_Bath \u0026lt;int\u0026gt;, Bedroom_AbvGr \u0026lt;int\u0026gt;, Kitchen_AbvGr \u0026lt;int\u0026gt;,\r#\u0026gt; # Kitchen_Qual \u0026lt;fct\u0026gt;, TotRms_AbvGrd \u0026lt;int\u0026gt;, Functional \u0026lt;fct\u0026gt;,\r#\u0026gt; # Fireplaces \u0026lt;int\u0026gt;, Fireplace_Qu \u0026lt;fct\u0026gt;, Garage_Type \u0026lt;fct\u0026gt;,\r#\u0026gt; # Garage_Finish \u0026lt;fct\u0026gt;, Garage_Cars \u0026lt;dbl\u0026gt;, Garage_Area \u0026lt;dbl\u0026gt;,\r#\u0026gt; # Garage_Qual \u0026lt;fct\u0026gt;, Garage_Cond \u0026lt;fct\u0026gt;, Paved_Drive \u0026lt;fct\u0026gt;,\r#\u0026gt; # Wood_Deck_SF \u0026lt;int\u0026gt;, Open_Porch_SF \u0026lt;int\u0026gt;, Enclosed_Porch \u0026lt;int\u0026gt;,\r#\u0026gt; # Three_season_porch \u0026lt;int\u0026gt;, Screen_Porch \u0026lt;int\u0026gt;, Pool_Area \u0026lt;int\u0026gt;,\r#\u0026gt; # Pool_QC \u0026lt;fct\u0026gt;, Fence \u0026lt;fct\u0026gt;, Misc_Feature \u0026lt;fct\u0026gt;, Misc_Val \u0026lt;int\u0026gt;,\r#\u0026gt; # Mo_Sold \u0026lt;int\u0026gt;, Year_Sold \u0026lt;int\u0026gt;, Sale_Type \u0026lt;fct\u0026gt;, Sale_Condition \u0026lt;fct\u0026gt;,\r#\u0026gt; # Sale_Price \u0026lt;int\u0026gt;, Longitude \u0026lt;dbl\u0026gt;, Latitude \u0026lt;dbl\u0026gt;\rImmediately the amount of columns jumps out. I will start with cleaning the names and, because I think there’s going to be quite a lot of overlap between some of the variables, I’m going to select some that I think will be useful in predicting house price. Just glimpsing over the data, let’s keep longitude, latitude, lot_area, neighborhood (though may overlap with long / lat), year_sold and overall_qual.\names_e \u0026lt;- ames %\u0026gt;%\rjanitor::clean_names() %\u0026gt;%\rselect(longitude, latitude, lot_area,\rneighborhood, year_sold, overall_qual, sale_price)\rglimpse(ames_e)\r#\u0026gt; Rows: 2,930\r#\u0026gt; Columns: 7\r#\u0026gt; $ longitude \u0026lt;dbl\u0026gt; -93.61975, -93.61976, -93.61939, -93.61732, -93.63893,...\r#\u0026gt; $ latitude \u0026lt;dbl\u0026gt; 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 42.0...\r#\u0026gt; $ lot_area \u0026lt;int\u0026gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005, 5...\r#\u0026gt; $ neighborhood \u0026lt;fct\u0026gt; North_Ames, North_Ames, North_Ames, North_Ames, Gilber...\r#\u0026gt; $ year_sold \u0026lt;int\u0026gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, ...\r#\u0026gt; $ overall_qual \u0026lt;fct\u0026gt; Above_Average, Average, Above_Average, Good, Average, ...\r#\u0026gt; $ sale_price \u0026lt;int\u0026gt; 215000, 105000, 172000, 244000, 189900, 195500, 213500...\rLet’s plot distribution of sale_price to see if it there’s anything weird going on:\nggplot(ames_e, aes(sale_price)) +\rgeom_histogram(bins = 30, alpha = .6, fill = \u0026quot;midnightblue\u0026quot;) +\rscale_x_continuous(labels = scales::dollar_format()) +\rtheme_ipsum_rc() +\rlabs(y = NULL,\rx = \u0026quot;Cost\u0026quot;,\rtitle = \u0026quot;Distribution of house prices in Ames\u0026quot;)\rWe can see there are a few extreme cases where house prices are \u0026gt; $400k. Additionally, because of the high numbers, it might be good to log10 but I will try without first. It is interesting to me though that these are mainly in one neighbourhood (Northridge Heights) but when looking at the distribution of that specific neighbourhood, it is still similar to that of the overall dataset!\nAnyway, there is not much else to explore here. Instead, let’s look at the Tidymodel process.\n\rModel 1\rI’m only going to be using regression models for this article and the aim is to see if we can predict house prices based on the predictors we picked. I’ll start by splitting the data and also create some folds so we don’t have to touch the testing data until later.\nSplitting\rames_split \u0026lt;- initial_split(ames_e)\rames_train \u0026lt;- training(ames_split)\rames_test \u0026lt;- testing(ames_split)\rset.seed(123)\rames_folds \u0026lt;- vfold_cv(ames_train)\rNeed to remember that the general approach to Tidymodels is:\nRecipe\n\rModel\n\rWorkflow\n\rFit\n\rRepeat\n\r\rIn terms of recipe, I don’t think there is much to do here currently. We may have to specify a recipe to log our sale_price variable but let’s try it without first. Usually though, a recipe can allow us to define many steps that make things like normalising / centering data easier. Can also help with zero variables.\nrf_recipe \u0026lt;-\rrecipe(sale_price ~ .,\rdata = ames_train)\rNow we can have a look at the model. We’ll start with the random forest which has 3 hyper-parameters we can tune: mtry, trees, and min_n. For the engine, we’ll use ranger which is a “fast implementation of random forests or recursive partitioning particularly suited for high dimensional data”. It supports both classification and regression so we have to specify that we’re doing regression!\nrf_model \u0026lt;-\rrand_forest() %\u0026gt;%\rset_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;%\rset_mode(\u0026quot;regression\u0026quot;)\rrf_model %\u0026gt;% translate()\r#\u0026gt; Random Forest Model Specification (regression)\r#\u0026gt; #\u0026gt; Computational engine: ranger #\u0026gt; #\u0026gt; Model fit template:\r#\u0026gt; ranger::ranger(formula = missing_arg(), data = missing_arg(), #\u0026gt; case.weights = missing_arg(), num.threads = 1, verbose = FALSE, #\u0026gt; seed = sample.int(10^5, 1))\rNow it’s time for the workflow. Workflow in Tidymodels helps us create an object that binds together the pre-processing, modeling, AND post-processing requests. This makes it much easier to oversee what exactly is going to happen in a model (when combined with a recipe) and additionally, we can use commands like update_workflow() for different models.\nrf_wf \u0026lt;-\rworkflow() %\u0026gt;%\radd_recipe(rf_recipe) %\u0026gt;%\radd_model(rf_model)\rsummary(rf_wf)\r#\u0026gt; Length Class Mode #\u0026gt; pre 2 stage_pre list #\u0026gt; fit 2 stage_fit list #\u0026gt; post 1 stage_post list #\u0026gt; trained 1 -none- logical\rLastly we fit and see what the metrics look like for the folds. This way we can see if we want to attempt trying it on the testing model or if we further want to adjust our hyper-parameters (maybe with a grid).\nset.seed(123)\rrf_fit \u0026lt;-\rrf_wf %\u0026gt;%\rfit_resamples(resamples = ames_folds)\rrf_fit %\u0026gt;%\rcollect_metrics()\r#\u0026gt; # A tibble: 2 x 5\r#\u0026gt; .metric .estimator mean n std_err\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 rmse standard 31742. 10 1201. #\u0026gt; 2 rsq standard 0.842 10 0.00921\rThe R2 looks okay for the training set - sitting at 0.83 and we have an RMSE of $32k. Let’s see what it looks like on the testing data! I think we should have log10’d the sale_price but let’s go ahead and test it on the testing data and then for model 2 I will use log10.\n\rPredict\rset.seed(123)\rrf_last_fit \u0026lt;-\rrf_wf %\u0026gt;%\rlast_fit(split = ames_split)\rresults \u0026lt;- rf_last_fit %\u0026gt;% collect_metrics()\rrf_last_fit %\u0026gt;%\rcollect_predictions() %\u0026gt;%\rggplot(aes(.pred, sale_price)) +\rgeom_abline(col = \u0026quot;green\u0026quot;, lty = 2) +\rgeom_point(alpha = .4, colour = \u0026quot;midnightblue\u0026quot;) +\rannotate(\u0026quot;text\u0026quot;, x = Inf, y = Inf, hjust = 1.5, vjust = 2, label = paste(\u0026quot;RMSE: \u0026quot;, round(results$.estimate[1], 4))) +\rlabs(title = \u0026quot;Random tree model\u0026quot;,\rsubtitle = \u0026quot;No hyperparameters tuned, ranger engine\u0026quot;) +\rscale_x_continuous(labels = scales::dollar_format()) +\rscale_y_continuous(labels = scales::dollar_format())\rThe RMSE for the test set is $33k which means that on average, the predictions are about 33k off the actuals. The R2 looks about similar to the training set though, and there’s not SUCH a big difference between training and testing RMSE as to cause alarm. Now let’s try something similar but with log10. I attempted to do the hyper-parameter tuning but the amount of time it takes (in terms of computing), I’d rather leave it to a future post. Let’s log10 the sale_price column to see if that makes a difference.\n\r\rModel 2\rWe have already defined the model above and for now, won’t edit any of the args so let’s try and simply update our recipe and add that to our workflow. Let’s have another look at how our recipe, rf_recipe is defined:\nrf_model\r#\u0026gt; Random Forest Model Specification (regression)\r#\u0026gt; #\u0026gt; Computational engine: ranger\rrf_recipe\r#\u0026gt; Data Recipe\r#\u0026gt; #\u0026gt; Inputs:\r#\u0026gt; #\u0026gt; role #variables\r#\u0026gt; outcome 1\r#\u0026gt; predictor 6\rAll I did initially was put in the formula (which I could’ve done with add_formula in workflow as there weren’t any recipe stats) so I just need to add a step. Luckily, the recipe package has very straightforward function names so it should be clear.\nrf_log10_recipe \u0026lt;-\rrecipe(sale_price ~ .,\rdata = ames_train) %\u0026gt;%\rstep_log(sale_price, base = 10)\rAnd now when I run rf_log10_recipe it should say what operation it’s going to run once I’ve added it to the workflow and fitted it.\nrf_log10_recipe\r#\u0026gt; Data Recipe\r#\u0026gt; #\u0026gt; Inputs:\r#\u0026gt; #\u0026gt; role #variables\r#\u0026gt; outcome 1\r#\u0026gt; predictor 6\r#\u0026gt; #\u0026gt; Operations:\r#\u0026gt; #\u0026gt; Log transformation on sale_price\rIn the course linked in the introduction, Julia demonstrates how easy it is to update existing workflows with update_model() and update_recipe(). In this case, I’m just updating the recipe so let’s see how it works.\nrf_log10_wf \u0026lt;- rf_wf %\u0026gt;%\rupdate_recipe(rf_log10_recipe)\rNow rather than having to go back and also add model to the workflow (and whatever else if we had a more complicated workflow), we simply call the updated recipe and then we are ready to fit another model. This makes it a lot easier to tune models repeatedly and to test them for accuracy.\nset.seed(123)\rrf_log10_fit \u0026lt;-\rrf_log10_wf %\u0026gt;%\rfit_resamples(resamples = ames_folds)\rrf_log10_fit %\u0026gt;%\rcollect_metrics()\r#\u0026gt; # A tibble: 2 x 5\r#\u0026gt; .metric .estimator mean n std_err\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 rmse standard 0.0771 10 0.00231\r#\u0026gt; 2 rsq standard 0.812 10 0.00775\rThe R2 has actually gone down a bit but well, the focus is on the routine, not the outcome (here)! Let’s fit on the test set and see how it looks.\nset.seed(123)\rrf_log10_last_fit \u0026lt;-\rrf_log10_wf %\u0026gt;%\rlast_fit(split = ames_split)\rresults_log10 \u0026lt;- rf_log10_last_fit %\u0026gt;% collect_metrics()\rrf_log10_last_fit %\u0026gt;%\rcollect_predictions() %\u0026gt;%\rggplot(aes(.pred, sale_price)) +\rgeom_abline(col = \u0026quot;green\u0026quot;, lty = 2) +\rgeom_point(alpha = .4, colour = \u0026quot;midnightblue\u0026quot;) +\rannotate(\u0026quot;text\u0026quot;, x = Inf, y = Inf, hjust = 1.5, vjust = 2.5, label = paste(\u0026quot;RMSE: \u0026quot;, round(results_log10$.estimate[1], 4))) +\rlabs(title = \u0026quot;Random tree model\u0026quot;,\rsubtitle = \u0026quot;No hyperparameters tuned, ranger engine, log10 sale_price\u0026quot;) +\rscale_x_continuous(labels = scales::dollar_format()) +\rscale_y_continuous(labels = scales::dollar_format())\rUnfortunately because we changed the parameters it’s difficult to compare the two models. We could log10 the RMSE of the first model and see that it’s 4.51 but that’s not a good way. We could compare the R2 in which case the confidence in model 2 goes down as it’s lower. Anyway, lastly I want to implement a linear model that is described on the Tidymodels blog. This one will also use log10 so we CAN compare the RMSE.\n\rModel 3\rI want to use glmnet which relies on regularization so will need to center and scale the predictors first. I’m updating the recipe AND the model this time so I’m not going to use update_recipe() or update_model(), I’ll just create new ones altogether.\nglm_rec \u0026lt;- recipe(sale_price ~.,\rdata = ames_train) %\u0026gt;%\rstep_other(neighborhood) %\u0026gt;%\rstep_dummy(all_nominal()) %\u0026gt;%\rstep_center(all_predictors()) %\u0026gt;%\rstep_scale(all_predictors()) %\u0026gt;%\rstep_log(sale_price, base = 10)\rA few additional things are added. First of all, step_other() is a great fallback step in case there is a category that appears in the test set but didn’t appear in the training set. Usually this would throw an error but Tidymodels will automatically create a category called ‘Other’ for those. Then there’s step_dummy() which converts our neighborhood and overall_qual columns to dummies. To see what the data looks like once it’s been processed:\njuice(prep(glm_rec)) %\u0026gt;%\rhead(5)\r#\u0026gt; # A tibble: 5 x 22\r#\u0026gt; longitude latitude lot_area year_sold sale_price neighborhood_Co~\r#\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 0.909 1.07 2.71 1.68 5.33 -0.318\r#\u0026gt; 2 0.909 1.02 0.187 1.68 5.02 -0.318\r#\u0026gt; 3 1.00 0.919 0.129 1.68 5.39 -0.318\r#\u0026gt; 4 0.165 1.45 0.463 1.68 5.28 -0.318\r#\u0026gt; 5 0.165 1.44 -0.0183 1.68 5.29 -0.318\r#\u0026gt; # ... with 16 more variables: neighborhood_Old_Town \u0026lt;dbl\u0026gt;,\r#\u0026gt; # neighborhood_Edwards \u0026lt;dbl\u0026gt;, neighborhood_Somerset \u0026lt;dbl\u0026gt;,\r#\u0026gt; # neighborhood_Northridge_Heights \u0026lt;dbl\u0026gt;, neighborhood_Gilbert \u0026lt;dbl\u0026gt;,\r#\u0026gt; # neighborhood_Sawyer \u0026lt;dbl\u0026gt;, neighborhood_other \u0026lt;dbl\u0026gt;,\r#\u0026gt; # overall_qual_Poor \u0026lt;dbl\u0026gt;, overall_qual_Fair \u0026lt;dbl\u0026gt;,\r#\u0026gt; # overall_qual_Below_Average \u0026lt;dbl\u0026gt;, overall_qual_Average \u0026lt;dbl\u0026gt;,\r#\u0026gt; # overall_qual_Above_Average \u0026lt;dbl\u0026gt;, overall_qual_Good \u0026lt;dbl\u0026gt;,\r#\u0026gt; # overall_qual_Very_Good \u0026lt;dbl\u0026gt;, overall_qual_Excellent \u0026lt;dbl\u0026gt;,\r#\u0026gt; # overall_qual_Very_Excellent \u0026lt;dbl\u0026gt;\rNow let’s go to step 2, creating the model:\nglm_model \u0026lt;-\rlinear_reg(penalty = 0.001, mixture = 0.5) %\u0026gt;%\rset_engine(\u0026quot;glmnet\u0026quot;)\rStep 3 and step 4, which is putting together the workflow and fitting the data:\nglm_wf \u0026lt;-\rworkflow() %\u0026gt;%\radd_recipe(glm_rec) %\u0026gt;%\radd_model(glm_model)\rset.seed(123)\rglm_fit \u0026lt;-\rglm_wf %\u0026gt;%\rfit_resamples(resamples = ames_folds)\rglm_fit %\u0026gt;%\rcollect_metrics()\r#\u0026gt; # A tibble: 2 x 5\r#\u0026gt; .metric .estimator mean n std_err\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 rmse standard 0.0943 10 0.00305\r#\u0026gt; 2 rsq standard 0.718 10 0.0170\rAnd finally predict. Then we’ll put together the metrics for the two models (and pretend the first one didn’t happen..) and see whether the glmnet or the tree performed best.\nset.seed(123)\rglm_last_fit \u0026lt;-\rglm_wf %\u0026gt;%\rlast_fit(split = ames_split)\rresults_glm \u0026lt;- glm_last_fit %\u0026gt;% collect_metrics()\rglm_last_fit %\u0026gt;%\rcollect_predictions() %\u0026gt;%\rggplot(aes(.pred, sale_price)) +\rgeom_abline(col = \u0026quot;green\u0026quot;, lty = 2) +\rgeom_point(alpha = .4, colour = \u0026quot;midnightblue\u0026quot;) +\rannotate(\u0026quot;text\u0026quot;, x = Inf, y = Inf, hjust = 1.5, vjust = 2.5, label = paste(\u0026quot;RMSE: \u0026quot;, round(results_glm$.estimate[1], 4))) +\rlabs(title = \u0026quot;Linear regression model\u0026quot;,\rsubtitle = \u0026quot;Variety of steps in recipe, glmnet engine\u0026quot;) +\rscale_x_continuous(labels = scales::dollar_format()) +\rscale_y_continuous(labels = scales::dollar_format())\r\rFinal\rNow let’s simply combine the prediction data for the two models and plot them side-by-side so we can see which one has performed best:\ntest_results \u0026lt;- rf_log10_last_fit %\u0026gt;%\rcollect_predictions() %\u0026gt;%\rrename(\u0026quot;rf\u0026quot; = .pred) %\u0026gt;%\rbind_cols(\rglm_last_fit %\u0026gt;%\rcollect_predictions() %\u0026gt;%\rrename(\u0026quot;glmnet\u0026quot; = .pred) %\u0026gt;%\rselect(\u0026quot;glmnet\u0026quot;)\r) %\u0026gt;%\rselect(-id, -.row)\rtest_results %\u0026gt;%\rgather(model, prediction, -sale_price) %\u0026gt;%\rggplot(aes(prediction, sale_price)) +\rgeom_abline(col = \u0026quot;green\u0026quot;, lty = 2) +\rgeom_point(col = \u0026quot;midnightblue\u0026quot;, alpha = .7) +\rfacet_wrap(~model) +\rcoord_fixed()\rWhen putting them side-by-side it looks like the tree model performed just slightly better. That said, they both did alright. In conclusion, Tidymodels has made the process that caret helped introduce even better. It’s very simple to fit a model, update a few steps, update the workflow, fit another model. Whilst in this article I used very simple models I can see it come to very good use when I’m trying to tune hyper-parameters with things like XGBoost!\n\r"
            }
        
    ,
        
            {
                "ref": "https://duncip.netlify.app/2020/05/10/broadway-continues-to-rise.html",
                "title": "Broadway continues to rise..",
                "section": "post",
                "date" : "2020.05.10",
                "body": "\nWeek 18 of TidyTuesday datasets focuses on Broadway and provides us with 4 separate datasets:\n\rgrosses which has the main bulk of the overall data, including weekly gross, show name, theatre name and average ticket price\n\rsynopses which shows the synopsis of the shows - going to forget about this for the sake of this article\n\rcpi which shows consumer price index - we can use this to adjust our money figures for inflation\n\rpre_1985_starts which lists the shows that started before 1985\n\r\rThe main dataset I’m interested in is grosses so let’s see what’s in there:\nglimpse(grosses_raw)\r## Rows: 47,524\r## Columns: 14\r## $ week_ending \u0026lt;date\u0026gt; 1985-06-09, 1985-06-09, 1985-06-09, 1985-06-0...\r## $ week_number \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\r## $ weekly_gross_overall \u0026lt;dbl\u0026gt; 3915937, 3915937, 3915937, 3915937, 3915937, 3...\r## $ show \u0026lt;chr\u0026gt; \u0026quot;42nd Street\u0026quot;, \u0026quot;A Chorus Line\u0026quot;, \u0026quot;Aren\u0026#39;t We All...\r## $ theatre \u0026lt;chr\u0026gt; \u0026quot;St. James Theatre\u0026quot;, \u0026quot;Sam S. Shubert Theatre\u0026quot;,...\r## $ weekly_gross \u0026lt;dbl\u0026gt; 282368, 222584, 249272, 95688, 61059, 255386, ...\r## $ potential_gross \u0026lt;lgl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...\r## $ avg_ticket_price \u0026lt;dbl\u0026gt; 30.42, 27.25, 33.75, 20.87, 20.78, 31.96, 28.3...\r## $ top_ticket_price \u0026lt;lgl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...\r## $ seats_sold \u0026lt;dbl\u0026gt; 9281, 8167, 7386, 4586, 2938, 7992, 10831, 567...\r## $ seats_in_theatre \u0026lt;dbl\u0026gt; 1655, 1472, 1088, 682, 684, 1018, 1336, 1368, ...\r## $ pct_capacity \u0026lt;dbl\u0026gt; 0.7010, 0.6935, 0.8486, 0.8405, 0.5369, 0.9813...\r## $ performances \u0026lt;dbl\u0026gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 0, 8...\r## $ previews \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0...\rThere’s a lot of great data in there - I want to create a few things including a patchwork that gives us a quick look into how Broadway shows have performed over the last 35 years, a dashboard (mainly because I’ve been wanting to try flexdashboard and tidymetrics!) and a quick race bar chart (though I don’t think this’ll be great.)\nCleaning the data\rFor me, the main columns of interest are week_ending, weekly_gross, weekly_gross_overall, avg_ticket_price, seats_sold, pct_capacity and we’ll have to use performances and previews to clean up the data. I also want to use the tidymetrics package to add in some additional date columns like quarters, years, and would like a rolling average (not sure how long yet). To do that we need to rename the column week_ending to date and then use cross_by_periods. Before adding in any additional columns I want to clean the data though so let’s:\n\rRename week_ending to date\n\rMake sure there are no 0’s in any of the numerical columns as this will affect our average\n\rAccount for inflation (I’m going to cut off the data at the end of 2019 so will use last week of 2019)\n\r\rgrosses \u0026lt;- grosses_raw %\u0026gt;%\rrename(date = week_ending) %\u0026gt;%\rmutate_at(vars(weekly_gross:pct_capacity),\r~ ifelse(performances + previews == 0 | . == 0, NA, .))\r# quick check to see how many NAs there are now\rstr(map(grosses, ~ sum(is.na(.))))\r## List of 14\r## $ date : int 0\r## $ week_number : int 0\r## $ weekly_gross_overall: int 0\r## $ show : int 0\r## $ theatre : int 0\r## $ weekly_gross : int 125\r## $ potential_gross : int 47524\r## $ avg_ticket_price : int 125\r## $ top_ticket_price : int 47524\r## $ seats_sold : int 125\r## $ seats_in_theatre : int 88\r## $ pct_capacity : int 133\r## $ performances : int 0\r## $ previews : int 0\rWith the easy bits done, we need to account for inflation. This requires a few steps: dividing all the CPIs by the picked date (December 2019), converting weekly dates in grosses to monthly so we can merge the two together, mutate all of the columns in grosses that use dollars to be updated with the inflation rate.\ncpi \u0026lt;- cpi_raw %\u0026gt;%\rmutate(dec_2019_dollars = cpi[year_month == \u0026quot;2019-12-01\u0026quot;] / cpi)\rgrosses_clean \u0026lt;- grosses %\u0026gt;%\rfilter(year(date) \u0026gt; 1985 \u0026amp; year(date) \u0026lt; 2020) %\u0026gt;%\rmutate(year_month = floor_date(date, unit = \u0026quot;month\u0026quot;)) %\u0026gt;% left_join(cpi, by = \u0026quot;year_month\u0026quot;) %\u0026gt;% mutate_at(\rvars(\rweekly_gross_overall,\rweekly_gross,\rpotential_gross,\ravg_ticket_price,\rtop_ticket_price\r),\r~ . * dec_2019_dollars\r) %\u0026gt;%\rselect(-potential_gross, -top_ticket_price, -c(year_month:dec_2019_dollars)) %\u0026gt;%\rmutate(year = year(date))\r\rExploring\rNow that we’ve done the initial data setup it’d be interesting to look at a few things. In particular I’d like to see:\n\rHow my favourite shows have performed over the years (Lion King \u0026amp; Phantom)\n\rHas Broadway become more popular? In terms of both number of plays and attendance\n\rWhat’s the price increase like (have adjusted for inflation above)\n\rHow’s the annual gross revenue?\n\r\rAs mentioned before there are four metrics that can help us assess the popularity of Broadway shows over the last 35 years. Starting with the top-left plot there’s been a clear year-on-year growth with a peak happening in 2018. A few reasons for the decrease in growth in 2019 include Bruce Springsteen’s concert closed in 2018 and shows like Anastasia brought in a lot less money (44m to 9m for Anastaria). We’d expect revenue to go up as number of performances goes up but the fill shows that’s not necessarily the case. 2008, for example, had a lot of performances but that growth wasn’t reflected in revenue.\nThe top-right chart, seats sold, shows the average has gone up considerably. Can see a clear shift beginning in 2015 which is when Hamilton opened and as both Hamilton and Book of Mormon have been consistently sold out since the start of their run, it’s pushing number of seats sold up. It’s also interesting that The Lion King, despite it’s long run, is still consistently selling out 96-97% of its capacity.\nThe bottom left chart looks at the range of average ticket prices for shows throughout the years and Hamilton’s high prices (as shown further down in the post) are pushing up the averages. Most of the outliers belong to Hamilton and Book of Mormon, both of which charge much higher than average prices (and have crazy top prices!). The bottom right is a simple plot showing the number of unique shows has risen each year, showing there’s been an expansion throughout the years to accommodate all of these.\nJust to emphasise that more performances generally mean more money.. but not always (as in 2008):\nggplot(performances_and_shows, aes(x = yearly_gross,\ry = number_of_performances)) +\rgeom_point() + labs(x = \u0026quot;# of shows\u0026quot;,\ry = \u0026quot;yearly revenue\u0026quot;,\rtitle = \u0026quot;More performances, more money?\u0026quot;)\r\rSpecific shows\rAfter looking more into what made Broadway grow over time I found that the top10 shows were contributing a lot to the overall growth. This is especially true for Hamilton and Book of Mormon, two shows that have driven up the average ticket price for shows and have also set a gold standard for selling out consistently! I wanted to create a quick dashboard with flexdashboard so the graphs below will be reflected in that. You can find the dashboard here:\rlink to dashboard\nGoing to start with adding in the periods and rolling averages using tidymetrics to see if it’ll give us some good insights. As shown by David Robinson in a previous tidy tuesday (think it was on beer!) we can use both cross_by_dimensions which will add an ‘All’ to our data (in this case in our show column) and cross_by_periods where we can determine what periods to add into the data and what kind of window to summarise them by:\ntop_shows \u0026lt;- grosses_clean %\u0026gt;%\rgroup_by(show) %\u0026gt;%\rsummarise(total_gross = sum(weekly_gross, na.rm = TRUE)) %\u0026gt;%\rarrange(desc(total_gross)) %\u0026gt;%\rungroup() %\u0026gt;%\rhead(10) %\u0026gt;%\rselect(show) %\u0026gt;%\rpull()\rgrosses_summarised \u0026lt;- grosses_clean %\u0026gt;%\rfilter(show %in% top_shows) %\u0026gt;%\rcross_by_dimensions(show) %\u0026gt;%\rcross_by_periods(c(\u0026quot;month\u0026quot;, \u0026quot;quarter\u0026quot;, \u0026quot;year\u0026quot;),\rwindows = 28) %\u0026gt;% summarise(\rusd_gross = sum(weekly_gross, na.rm = TRUE),\ravg_ticket_price = mean(avg_ticket_price, na.rm = TRUE),\rnb_seats_sold = sum(seats_sold, na.rm = TRUE),\rpct_capacity = mean(pct_capacity, na.rm = TRUE)\r) %\u0026gt;%\rungroup() %\u0026gt;%\rmutate(year = year(date))\rgrosses_summarised %\u0026gt;%\rfilter(period == \u0026quot;quarter\u0026quot;,\rshow != \u0026quot;All\u0026quot;) %\u0026gt;%\rmutate(show = fct_reorder(show, avg_ticket_price)) %\u0026gt;%\rggplot(aes(x = avg_ticket_price,\ry = show,\rfill = show)) +\rgeom_density_ridges(scale = 4, size = 0, alpha = 0.7) +\rscale_fill_igv() +\rguides(fill = \u0026quot;none\u0026quot;) +\rlabs(x = \u0026quot;Average ticket price\u0026quot;, y = \u0026quot;Show\u0026quot;,\rtitle = \u0026quot;Distribution of ticket prices for top 10 Broadway shows\u0026quot;) +\rtheme(\rplot.title = element_text(hjust = 1.5)\r)\rHamilton is by far the most expensive show on average which explains why it’s already in the top 10 highest grossing theatres despite only opening in 2015. I’m guessing it’s also going to have one of the highest attendance rates so we’ll have a look at that after visualising total gross (which I already calculated above!):\ngrosses_summarised %\u0026gt;%\rfilter(show != \u0026quot;All\u0026quot;,\rperiod == \u0026quot;year\u0026quot;) %\u0026gt;% mutate(show = fct_reorder(show, usd_gross)) %\u0026gt;% ggplot(aes(x = date,\ry = usd_gross,\rfill = show)) +\rgeom_col() +\rcoord_flip() +\rscale_fill_igv() +\rscale_x_date(date_breaks = \u0026quot;5 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) +\rscale_y_continuous(labels = scales::dollar_format()) +\rtheme(\rlegend.text = element_text(size = rel(0.8)),\rlegend.title = element_blank(),\rlegend.position = \u0026quot;bottom\u0026quot;\r) +\rlabs(x = NULL,\ry = NULL,\rtitle = \u0026quot;Total revenue by Broadway show\u0026quot;)\rIt’s very obvious here the astronominal amounts Hamilton is bringing in. It slowly pops up in its first year but then continuously grows until 2019 where it slightly starts dropping off. Now that we have a stacked bar plot it also becomes clear where certain shows dropped off like Cats, which closed in 2000 despite still pulling in lots of money. It also had a reboot in 2016/2017 where people flocked to see it! The Book of Mormon is another relatively new show which has been massively successful and just like Hamilton, has a very high price tag.\ngrosses_summarised %\u0026gt;%\rfilter(period == \u0026quot;quarter\u0026quot;,\rshow != \u0026quot;All\u0026quot;) %\u0026gt;%\rggplot(aes(x = date,\ry = pct_capacity,\rcolour = show)) +\rgeom_line(size = 1) +\rscale_y_continuous(labels = scales::percent_format()) +\rexpand_limits(y = 0) +\rscale_color_igv() +\rlabs(x = \u0026quot;\u0026quot;,\ry = \u0026quot;seats sold\u0026quot;,\rtitle = \u0026quot;Average % of sold seats per show\u0026quot;) +\rtheme(\rlegend.text = element_text(size = rel(0.8)),\rlegend.title = element_blank(),\rlegend.position = \u0026quot;bottom\u0026quot;\r)\rQuite hard to interpret in a static plot with 10 data points but three things stand out immediately: 1) Hamilton consistently has above 100% seat capacity, 2) Book of Mormon has the same, and has been selling out since it started its run, 3) The Lion King has a surprisingly varying attendance rate. That said, out of all the long running shows, it still manages to sell out most of the time.\nNow that we have our main graphs I’ll add them to flexdashboard and add a few score cards to show overall aggregations of the main metrics used. Lastly I wanted to add a quick bar chart race for top 10 shows throughout the years. I didn’t think it would turn out great (it didn’t) but the method might be interesting:\n\rBar chart race\rshow_yearly_gross \u0026lt;- grosses_clean %\u0026gt;%\rselect(date, show, weekly_gross) %\u0026gt;%\rmutate(year = year(date)) %\u0026gt;%\rgroup_by(year, show) %\u0026gt;%\rmutate(yearly_gross = sum(weekly_gross)) %\u0026gt;%\rungroup() %\u0026gt;%\rdistinct(year, show, yearly_gross)\ryearly_top10_shows_rev \u0026lt;- show_yearly_gross %\u0026gt;%\rgroup_by(year) %\u0026gt;%\rmutate(rank = rank(-yearly_gross),\rValue_rel = yearly_gross/yearly_gross[rank==1], Value_lbl = paste0(\u0026quot; \u0026quot;, round(yearly_gross/1e7))) %\u0026gt;%\rgroup_by(show) %\u0026gt;%\rfilter(rank \u0026lt;= 10) %\u0026gt;%\rungroup()\rWe now have all the data we need to set up a static plot for every single year:\nshows_static \u0026lt;- ggplot(yearly_top10_shows_rev,\raes(x = rank,\rgroup = show,\rfill = as.factor(show),\rcolour = as.factor(show))) +\rgeom_tile(aes(y = yearly_gross / 2,\rheight = yearly_gross,\rwidth = 0.9), alpha = 0.8, colour = NA) +\rgeom_text(aes(y = 0, label = paste(show, \u0026quot; \u0026quot;)),\rvjust = 0.2, hjust = 1) +\rcoord_flip(clip = \u0026quot;off\u0026quot;, expand = FALSE) +\rscale_y_continuous(labels = scales::comma) +\rscale_x_reverse() +\rguides(color = FALSE, fill = FALSE) +\rtheme(axis.line=element_blank(),\raxis.text.x=element_blank(),\raxis.text.y=element_blank(),\raxis.ticks=element_blank(),\raxis.title.x=element_blank(),\raxis.title.y=element_blank(),\rlegend.position=\u0026quot;none\u0026quot;,\rpanel.background=element_blank(),\rpanel.border=element_blank(),\rpanel.grid.major=element_blank(),\rpanel.grid.minor=element_blank(),\rpanel.grid.major.x = element_line( size=.1, color=\u0026quot;grey\u0026quot; ),\rpanel.grid.minor.x = element_line( size=.1, color=\u0026quot;grey\u0026quot; ),\rplot.title=element_text(size=25, hjust=0.5, face=\u0026quot;bold\u0026quot;, colour=\u0026quot;grey\u0026quot;, vjust=-1),\rplot.subtitle=element_text(size=18, hjust=0.5, face=\u0026quot;italic\u0026quot;, color=\u0026quot;grey\u0026quot;),\rplot.caption =element_text(size=8, hjust=0.5, face=\u0026quot;italic\u0026quot;, color=\u0026quot;grey\u0026quot;),\rplot.background=element_blank(),\rplot.margin = margin(2,2, 2, 4, \u0026quot;cm\u0026quot;))\rshows_static +\rtransition_time(year)\rlibrary(gifski)\ranim = shows_static + transition_states(year,\rtransition_length = 8, state_length = 68) +\rview_follow(fixed_x = TRUE) +\rlabs(title = \u0026quot;Revenue per year : {closest_state}\u0026quot;,\rsubtitle = \u0026quot;Top 10 Shows\u0026quot;)\ranimate(anim, 300, fps = 20, width = 1200, height = 1000, renderer = gifski_renderer(\u0026quot;yearly_top10_shows_br.gif\u0026quot;))\rbarchart race of highest grossing shows\n\rConclusion\rI really liked this TidyTuesday dataset, there’s a lot of data to play with. My main takeaway from all this:\n\rI need to go see Hamilton and Book of Mormon - it’s incredible how they have both managed to ‘change the game’. The fact that their average seat price is so high and they are still constantly over capacity shows that the hype around these shows isn’t dying down\n\rThe Lion King and Wicked are both still doing very well. Wicked especially is impressive to me due to its very long run!\n\rTidymetrics is incredible - it makes it so much easier to quickly aggregate metrics\n\rIt’s really easy to set up a flexdashboard and I’ll definitely be using lots more of Patchwork in the future\n\r\rAll of the code can be found in the blog section on my GitHub\n\r\r"
            }
        
    
]