[
    
        
            {
                "ref": "/2020/06/08/exploring-tidymodels-with-regression/",
                "title": "Exploring Tidymodels with regression",
                "section": "post",
                "date" : "2020.06.08",
                "body": "\nMost of the things below are learnt through materials provided by Julia Silge (her course) and Alison Hill’s course at rstudio::conf(2020). It’s mainly to have a reference for myself in terms of what the Tidymodels workflow is as it makes modeling much easier. I’m not aiming for accuracy of models here (and the comparisons are not great when using RMSE considering different processes), instead, I just want to document the Tidymodels process so I can easily implement it in the future.\nExplore\rLet’s have a look at the data - I don’t want to do too much data manipulation here as want to focus on the aspect of a simple Tidymodels workflow!\names %\u0026gt;%\rhead(5)\r#\u0026gt; # A tibble: 5 x 81\r#\u0026gt; MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape\r#\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 One_Story_~ Resident~ 141 31770 Pave No_A~ Slightly~\r#\u0026gt; 2 One_Story_~ Resident~ 80 11622 Pave No_A~ Regular #\u0026gt; 3 One_Story_~ Resident~ 81 14267 Pave No_A~ Slightly~\r#\u0026gt; 4 One_Story_~ Resident~ 93 11160 Pave No_A~ Regular #\u0026gt; 5 Two_Story_~ Resident~ 74 13830 Pave No_A~ Slightly~\r#\u0026gt; # ... with 74 more variables: Land_Contour \u0026lt;fct\u0026gt;, Utilities \u0026lt;fct\u0026gt;,\r#\u0026gt; # Lot_Config \u0026lt;fct\u0026gt;, Land_Slope \u0026lt;fct\u0026gt;, Neighborhood \u0026lt;fct\u0026gt;, Condition_1 \u0026lt;fct\u0026gt;,\r#\u0026gt; # Condition_2 \u0026lt;fct\u0026gt;, Bldg_Type \u0026lt;fct\u0026gt;, House_Style \u0026lt;fct\u0026gt;, Overall_Qual \u0026lt;fct\u0026gt;,\r#\u0026gt; # Overall_Cond \u0026lt;fct\u0026gt;, Year_Built \u0026lt;int\u0026gt;, Year_Remod_Add \u0026lt;int\u0026gt;,\r#\u0026gt; # Roof_Style \u0026lt;fct\u0026gt;, Roof_Matl \u0026lt;fct\u0026gt;, Exterior_1st \u0026lt;fct\u0026gt;, Exterior_2nd \u0026lt;fct\u0026gt;,\r#\u0026gt; # Mas_Vnr_Type \u0026lt;fct\u0026gt;, Mas_Vnr_Area \u0026lt;dbl\u0026gt;, Exter_Qual \u0026lt;fct\u0026gt;, Exter_Cond \u0026lt;fct\u0026gt;,\r#\u0026gt; # Foundation \u0026lt;fct\u0026gt;, Bsmt_Qual \u0026lt;fct\u0026gt;, Bsmt_Cond \u0026lt;fct\u0026gt;, Bsmt_Exposure \u0026lt;fct\u0026gt;,\r#\u0026gt; # BsmtFin_Type_1 \u0026lt;fct\u0026gt;, BsmtFin_SF_1 \u0026lt;dbl\u0026gt;, BsmtFin_Type_2 \u0026lt;fct\u0026gt;,\r#\u0026gt; # BsmtFin_SF_2 \u0026lt;dbl\u0026gt;, Bsmt_Unf_SF \u0026lt;dbl\u0026gt;, Total_Bsmt_SF \u0026lt;dbl\u0026gt;, Heating \u0026lt;fct\u0026gt;,\r#\u0026gt; # Heating_QC \u0026lt;fct\u0026gt;, Central_Air \u0026lt;fct\u0026gt;, Electrical \u0026lt;fct\u0026gt;, First_Flr_SF \u0026lt;int\u0026gt;,\r#\u0026gt; # Second_Flr_SF \u0026lt;int\u0026gt;, Low_Qual_Fin_SF \u0026lt;int\u0026gt;, Gr_Liv_Area \u0026lt;int\u0026gt;,\r#\u0026gt; # Bsmt_Full_Bath \u0026lt;dbl\u0026gt;, Bsmt_Half_Bath \u0026lt;dbl\u0026gt;, Full_Bath \u0026lt;int\u0026gt;,\r#\u0026gt; # Half_Bath \u0026lt;int\u0026gt;, Bedroom_AbvGr \u0026lt;int\u0026gt;, Kitchen_AbvGr \u0026lt;int\u0026gt;,\r#\u0026gt; # Kitchen_Qual \u0026lt;fct\u0026gt;, TotRms_AbvGrd \u0026lt;int\u0026gt;, Functional \u0026lt;fct\u0026gt;,\r#\u0026gt; # Fireplaces \u0026lt;int\u0026gt;, Fireplace_Qu \u0026lt;fct\u0026gt;, Garage_Type \u0026lt;fct\u0026gt;,\r#\u0026gt; # Garage_Finish \u0026lt;fct\u0026gt;, Garage_Cars \u0026lt;dbl\u0026gt;, Garage_Area \u0026lt;dbl\u0026gt;,\r#\u0026gt; # Garage_Qual \u0026lt;fct\u0026gt;, Garage_Cond \u0026lt;fct\u0026gt;, Paved_Drive \u0026lt;fct\u0026gt;,\r#\u0026gt; # Wood_Deck_SF \u0026lt;int\u0026gt;, Open_Porch_SF \u0026lt;int\u0026gt;, Enclosed_Porch \u0026lt;int\u0026gt;,\r#\u0026gt; # Three_season_porch \u0026lt;int\u0026gt;, Screen_Porch \u0026lt;int\u0026gt;, Pool_Area \u0026lt;int\u0026gt;,\r#\u0026gt; # Pool_QC \u0026lt;fct\u0026gt;, Fence \u0026lt;fct\u0026gt;, Misc_Feature \u0026lt;fct\u0026gt;, Misc_Val \u0026lt;int\u0026gt;,\r#\u0026gt; # Mo_Sold \u0026lt;int\u0026gt;, Year_Sold \u0026lt;int\u0026gt;, Sale_Type \u0026lt;fct\u0026gt;, Sale_Condition \u0026lt;fct\u0026gt;,\r#\u0026gt; # Sale_Price \u0026lt;int\u0026gt;, Longitude \u0026lt;dbl\u0026gt;, Latitude \u0026lt;dbl\u0026gt;\rImmediately the amount of columns jumps out. I will start with cleaning the names and, because I think there’s going to be quite a lot of overlap between some of the variables, I’m going to select some that I think will be useful in predicting house price. Just glimpsing over the data, let’s keep longitude, latitude, lot_area, neighborhood (though may overlap with long / lat), year_sold and overall_qual.\names_e \u0026lt;- ames %\u0026gt;%\rjanitor::clean_names() %\u0026gt;%\rselect(longitude, latitude, lot_area,\rneighborhood, year_sold, overall_qual, sale_price)\rglimpse(ames_e)\r#\u0026gt; Rows: 2,930\r#\u0026gt; Columns: 7\r#\u0026gt; $ longitude \u0026lt;dbl\u0026gt; -93.61975, -93.61976, -93.61939, -93.61732, -93.63893,...\r#\u0026gt; $ latitude \u0026lt;dbl\u0026gt; 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 42.0...\r#\u0026gt; $ lot_area \u0026lt;int\u0026gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005, 5...\r#\u0026gt; $ neighborhood \u0026lt;fct\u0026gt; North_Ames, North_Ames, North_Ames, North_Ames, Gilber...\r#\u0026gt; $ year_sold \u0026lt;int\u0026gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, ...\r#\u0026gt; $ overall_qual \u0026lt;fct\u0026gt; Above_Average, Average, Above_Average, Good, Average, ...\r#\u0026gt; $ sale_price \u0026lt;int\u0026gt; 215000, 105000, 172000, 244000, 189900, 195500, 213500...\rLet’s plot distribution of sale_price to see if it there’s anything weird going on:\nggplot(ames_e, aes(sale_price)) +\rgeom_histogram(bins = 30, alpha = .6, fill = \u0026quot;midnightblue\u0026quot;) +\rscale_x_continuous(labels = scales::dollar_format()) +\rtheme_ipsum_rc() +\rlabs(y = NULL,\rx = \u0026quot;Cost\u0026quot;,\rtitle = \u0026quot;Distribution of house prices in Ames\u0026quot;)\rWe can see there are a few extreme cases where house prices are \u0026gt; $400k. Additionally, because of the high numbers, it might be good to log10 but I will try without first. It is interesting to me though that these are mainly in one neighbourhood (Northridge Heights) but when looking at the distribution of that specific neighbourhood, it is still similar to that of the overall dataset!\nAnyway, there is not much else to explore here. Instead, let’s look at the Tidymodel process.\n\rModel 1\rI’m only going to be using regression models for this article and the aim is to see if we can predict house prices based on the predictors we picked. I’ll start by splitting the data and also create some folds so we don’t have to touch the testing data until later.\nSplitting\rames_split \u0026lt;- initial_split(ames_e)\rames_train \u0026lt;- training(ames_split)\rames_test \u0026lt;- testing(ames_split)\rset.seed(123)\rames_folds \u0026lt;- vfold_cv(ames_train)\rNeed to remember that the general approach to Tidymodels is:\nRecipe\n\rModel\n\rWorkflow\n\rFit\n\rRepeat\n\r\rIn terms of recipe, I don’t think there is much to do here currently. We may have to specify a recipe to log our sale_price variable but let’s try it without first. Usually though, a recipe can allow us to define many steps that make things like normalising / centering data easier. Can also help with zero variables.\nrf_recipe \u0026lt;-\rrecipe(sale_price ~ .,\rdata = ames_train)\rNow we can have a look at the model. We’ll start with the random forest which has 3 hyper-parameters we can tune: mtry, trees, and min_n. For the engine, we’ll use ranger which is a “fast implementation of random forests or recursive partitioning particularly suited for high dimensional data”. It supports both classification and regression so we have to specify that we’re doing regression!\nrf_model \u0026lt;-\rrand_forest() %\u0026gt;%\rset_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;%\rset_mode(\u0026quot;regression\u0026quot;)\rrf_model %\u0026gt;% translate()\r#\u0026gt; Random Forest Model Specification (regression)\r#\u0026gt; #\u0026gt; Computational engine: ranger #\u0026gt; #\u0026gt; Model fit template:\r#\u0026gt; ranger::ranger(formula = missing_arg(), data = missing_arg(), #\u0026gt; case.weights = missing_arg(), num.threads = 1, verbose = FALSE, #\u0026gt; seed = sample.int(10^5, 1))\rNow it’s time for the workflow. Workflow in Tidymodels helps us create an object that binds together the pre-processing, modeling, AND post-processing requests. This makes it much easier to oversee what exactly is going to happen in a model (when combined with a recipe) and additionally, we can use commands like update_workflow() for different models.\nrf_wf \u0026lt;-\rworkflow() %\u0026gt;%\radd_recipe(rf_recipe) %\u0026gt;%\radd_model(rf_model)\rsummary(rf_wf)\r#\u0026gt; Length Class Mode #\u0026gt; pre 2 stage_pre list #\u0026gt; fit 2 stage_fit list #\u0026gt; post 1 stage_post list #\u0026gt; trained 1 -none- logical\rLastly we fit and see what the metrics look like for the folds. This way we can see if we want to attempt trying it on the testing model or if we further want to adjust our hyper-parameters (maybe with a grid).\nset.seed(123)\rrf_fit \u0026lt;-\rrf_wf %\u0026gt;%\rfit_resamples(resamples = ames_folds)\rrf_fit %\u0026gt;%\rcollect_metrics()\r#\u0026gt; # A tibble: 2 x 5\r#\u0026gt; .metric .estimator mean n std_err\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 rmse standard 32527. 10 962. #\u0026gt; 2 rsq standard 0.832 10 0.0103\rThe R2 looks okay for the training set - sitting at 0.83 and we have an RMSE of $32k. Let’s see what it looks like on the testing data! I think we should have log10’d the sale_price but let’s go ahead and test it on the testing data and then for model 2 I will use log10.\n\rPredict\rset.seed(123)\rrf_last_fit \u0026lt;-\rrf_wf %\u0026gt;%\rlast_fit(split = ames_split)\rresults \u0026lt;- rf_last_fit %\u0026gt;% collect_metrics()\rrf_last_fit %\u0026gt;%\rcollect_predictions() %\u0026gt;%\rggplot(aes(.pred, sale_price)) +\rgeom_abline(col = \u0026quot;green\u0026quot;, lty = 2) +\rgeom_point(alpha = .4, colour = \u0026quot;midnightblue\u0026quot;) +\rannotate(\u0026quot;text\u0026quot;, x = Inf, y = Inf, hjust = 1.5, vjust = 2, label = paste(\u0026quot;RMSE: \u0026quot;, round(results$.estimate[1], 4))) +\rlabs(title = \u0026quot;Random tree model\u0026quot;,\rsubtitle = \u0026quot;No hyperparameters tuned, ranger engine\u0026quot;) +\rscale_x_continuous(labels = scales::dollar_format()) +\rscale_y_continuous(labels = scales::dollar_format())\rThe RMSE for the test set is $33k which means that on average, the predictions are about 33k off the actuals. The R2 looks about similar to the training set though, and there’s not SUCH a big difference between training and testing RMSE as to cause alarm. Now let’s try something similar but with log10. I attempted to do the hyper-parameter tuning but the amount of time it takes (in terms of computing), I’d rather leave it to a future post. Let’s log10 the sale_price column to see if that makes a difference.\n\r\rModel 2\rWe have already defined the model above and for now, won’t edit any of the args so let’s try and simply update our recipe and add that to our workflow. Let’s have another look at how our recipe, rf_recipe is defined:\nrf_model\r#\u0026gt; Random Forest Model Specification (regression)\r#\u0026gt; #\u0026gt; Computational engine: ranger\rrf_recipe\r#\u0026gt; Data Recipe\r#\u0026gt; #\u0026gt; Inputs:\r#\u0026gt; #\u0026gt; role #variables\r#\u0026gt; outcome 1\r#\u0026gt; predictor 6\rAll I did initially was put in the formula (which I could’ve done with add_formula in workflow as there weren’t any recipe stats) so I just need to add a step. Luckily, the recipe package has very straightforward function names so it should be clear.\nrf_log10_recipe \u0026lt;-\rrecipe(sale_price ~ .,\rdata = ames_train) %\u0026gt;%\rstep_log(sale_price, base = 10)\rAnd now when I run rf_log10_recipe it should say what operation it’s going to run once I’ve added it to the workflow and fitted it.\nrf_log10_recipe\r#\u0026gt; Data Recipe\r#\u0026gt; #\u0026gt; Inputs:\r#\u0026gt; #\u0026gt; role #variables\r#\u0026gt; outcome 1\r#\u0026gt; predictor 6\r#\u0026gt; #\u0026gt; Operations:\r#\u0026gt; #\u0026gt; Log transformation on sale_price\rIn the course linked in the introduction, Julia demonstrates how easy it is to update existing workflows with update_model() and update_recipe(). In this case, I’m just updating the recipe so let’s see how it works.\nrf_log10_wf \u0026lt;- rf_wf %\u0026gt;%\rupdate_recipe(rf_log10_recipe)\rNow rather than having to go back and also add model to the workflow (and whatever else if we had a more complicated workflow), we simply call the updated recipe and then we are ready to fit another model. This makes it a lot easier to tune models repeatedly and to test them for accuracy.\nset.seed(123)\rrf_log10_fit \u0026lt;-\rrf_log10_wf %\u0026gt;%\rfit_resamples(resamples = ames_folds)\rrf_log10_fit %\u0026gt;%\rcollect_metrics()\r#\u0026gt; # A tibble: 2 x 5\r#\u0026gt; .metric .estimator mean n std_err\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 rmse standard 0.0762 10 0.00209\r#\u0026gt; 2 rsq standard 0.811 10 0.00797\rThe R2 has actually gone down a bit but well, the focus is on the routine, not the outcome (here)! Let’s fit on the test set and see how it looks.\nset.seed(123)\rrf_log10_last_fit \u0026lt;-\rrf_log10_wf %\u0026gt;%\rlast_fit(split = ames_split)\rresults_log10 \u0026lt;- rf_log10_last_fit %\u0026gt;% collect_metrics()\rrf_log10_last_fit %\u0026gt;%\rcollect_predictions() %\u0026gt;%\rggplot(aes(.pred, sale_price)) +\rgeom_abline(col = \u0026quot;green\u0026quot;, lty = 2) +\rgeom_point(alpha = .4, colour = \u0026quot;midnightblue\u0026quot;) +\rannotate(\u0026quot;text\u0026quot;, x = Inf, y = Inf, hjust = 1.5, vjust = 2.5, label = paste(\u0026quot;RMSE: \u0026quot;, round(results_log10$.estimate[1], 4))) +\rlabs(title = \u0026quot;Random tree model\u0026quot;,\rsubtitle = \u0026quot;No hyperparameters tuned, ranger engine, log10 sale_price\u0026quot;) +\rscale_x_continuous(labels = scales::dollar_format()) +\rscale_y_continuous(labels = scales::dollar_format())\rUnfortunately because we changed the parameters it’s difficult to compare the two models. We could log10 the RMSE of the first model and see that it’s 4.51 but that’s not a good way. We could compare the R2 in which case the confidence in model 2 goes down as it’s lower. Anyway, lastly I want to implement a linear model that is described on the Tidymodels blog. This one will also use log10 so we CAN compare the RMSE.\n\rModel 3\rI want to use glmnet which relies on regularization so will need to center and scale the predictors first. I’m updating the recipe AND the model this time so I’m not going to use update_recipe() or update_model(), I’ll just create new ones altogether.\nglm_rec \u0026lt;- recipe(sale_price ~.,\rdata = ames_train) %\u0026gt;%\rstep_other(neighborhood) %\u0026gt;%\rstep_dummy(all_nominal()) %\u0026gt;%\rstep_center(all_predictors()) %\u0026gt;%\rstep_scale(all_predictors()) %\u0026gt;%\rstep_log(sale_price, base = 10)\rA few additional things are added. First of all, step_other() is a great fallback step in case there is a category that appears in the test set but didn’t appear in the training set. Usually this would throw an error but Tidymodels will automatically create a category called ‘Other’ for those. Then there’s step_dummy() which converts our neighborhood and overall_qual columns to dummies. To see what the data looks like once it’s been processed:\njuice(prep(glm_rec)) %\u0026gt;%\rhead(5)\r#\u0026gt; # A tibble: 5 x 22\r#\u0026gt; longitude latitude lot_area year_sold sale_price neighborhood_Co~\r#\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 0.894 1.06 3.01 1.69 5.33 -0.315\r#\u0026gt; 2 0.894 1.00 0.216 1.69 5.02 -0.315\r#\u0026gt; 3 0.908 0.985 0.582 1.69 5.24 -0.315\r#\u0026gt; 4 0.147 1.44 0.522 1.69 5.28 -0.315\r#\u0026gt; 5 0.147 1.43 -0.0114 1.69 5.29 -0.315\r#\u0026gt; # ... with 16 more variables: neighborhood_Old_Town \u0026lt;dbl\u0026gt;,\r#\u0026gt; # neighborhood_Edwards \u0026lt;dbl\u0026gt;, neighborhood_Somerset \u0026lt;dbl\u0026gt;,\r#\u0026gt; # neighborhood_Northridge_Heights \u0026lt;dbl\u0026gt;, neighborhood_Gilbert \u0026lt;dbl\u0026gt;,\r#\u0026gt; # neighborhood_Sawyer \u0026lt;dbl\u0026gt;, neighborhood_other \u0026lt;dbl\u0026gt;,\r#\u0026gt; # overall_qual_Poor \u0026lt;dbl\u0026gt;, overall_qual_Fair \u0026lt;dbl\u0026gt;,\r#\u0026gt; # overall_qual_Below_Average \u0026lt;dbl\u0026gt;, overall_qual_Average \u0026lt;dbl\u0026gt;,\r#\u0026gt; # overall_qual_Above_Average \u0026lt;dbl\u0026gt;, overall_qual_Good \u0026lt;dbl\u0026gt;,\r#\u0026gt; # overall_qual_Very_Good \u0026lt;dbl\u0026gt;, overall_qual_Excellent \u0026lt;dbl\u0026gt;,\r#\u0026gt; # overall_qual_Very_Excellent \u0026lt;dbl\u0026gt;\rNow let’s go to step 2, creating the model:\nglm_model \u0026lt;-\rlinear_reg(penalty = 0.001, mixture = 0.5) %\u0026gt;%\rset_engine(\u0026quot;glmnet\u0026quot;)\rStep 3 and step 4, which is putting together the workflow and fitting the data:\nglm_wf \u0026lt;-\rworkflow() %\u0026gt;%\radd_recipe(glm_rec) %\u0026gt;%\radd_model(glm_model)\rset.seed(123)\rglm_fit \u0026lt;-\rglm_wf %\u0026gt;%\rfit_resamples(resamples = ames_folds)\rglm_fit %\u0026gt;%\rcollect_metrics()\r#\u0026gt; # A tibble: 2 x 5\r#\u0026gt; .metric .estimator mean n std_err\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 rmse standard 0.0939 10 0.00364\r#\u0026gt; 2 rsq standard 0.714 10 0.0203\rAnd finally predict. Then we’ll put together the metrics for the two models (and pretend the first one didn’t happen..) and see whether the glmnet or the tree performed best.\nset.seed(123)\rglm_last_fit \u0026lt;-\rglm_wf %\u0026gt;%\rlast_fit(split = ames_split)\rresults_glm \u0026lt;- glm_last_fit %\u0026gt;% collect_metrics()\rglm_last_fit %\u0026gt;%\rcollect_predictions() %\u0026gt;%\rggplot(aes(.pred, sale_price)) +\rgeom_abline(col = \u0026quot;green\u0026quot;, lty = 2) +\rgeom_point(alpha = .4, colour = \u0026quot;midnightblue\u0026quot;) +\rannotate(\u0026quot;text\u0026quot;, x = Inf, y = Inf, hjust = 1.5, vjust = 2.5, label = paste(\u0026quot;RMSE: \u0026quot;, round(results_glm$.estimate[1], 4))) +\rlabs(title = \u0026quot;Linear regression model\u0026quot;,\rsubtitle = \u0026quot;Variety of steps in recipe, glmnet engine\u0026quot;) +\rscale_x_continuous(labels = scales::dollar_format()) +\rscale_y_continuous(labels = scales::dollar_format())\r\rFinal\rNow let’s simply combine the prediction data for the two models and plot them side-by-side so we can see which one has performed best:\ntest_results \u0026lt;- rf_log10_last_fit %\u0026gt;%\rcollect_predictions() %\u0026gt;%\rrename(\u0026quot;rf\u0026quot; = .pred) %\u0026gt;%\rbind_cols(\rglm_last_fit %\u0026gt;%\rcollect_predictions() %\u0026gt;%\rrename(\u0026quot;glmnet\u0026quot; = .pred) %\u0026gt;%\rselect(\u0026quot;glmnet\u0026quot;)\r) %\u0026gt;%\rselect(-id, -.row)\rtest_results %\u0026gt;%\rgather(model, prediction, -sale_price) %\u0026gt;%\rggplot(aes(prediction, sale_price)) +\rgeom_abline(col = \u0026quot;green\u0026quot;, lty = 2) +\rgeom_point(col = \u0026quot;midnightblue\u0026quot;, alpha = .7) +\rfacet_wrap(~model) +\rcoord_fixed()\rWhen putting them side-by-side it looks like the tree model performed just slightly better. That said, they both did alright. In conclusion, Tidymodels has made the process that caret helped introduce even better. It’s very simple to fit a model, update a few steps, update the workflow, fit another model. Whilst in this article I used very simple models I can see it come to very good use when I’m trying to tune hyper-parameters with things like XGBoost!\n\r"
            }
        
    ,
        
            {
                "ref": "/2020/05/10/broadway-continues-to-rise/",
                "title": "Broadway continues to rise..",
                "section": "post",
                "date" : "2020.05.10",
                "body": "\nWeek 18 of TidyTuesday datasets focuses on Broadway and provides us with 4 separate datasets:\n\rgrosses which has the main bulk of the overall data, including weekly gross, show name, theatre name and average ticket price\n\rsynopses which shows the synopsis of the shows - going to forget about this for the sake of this article\n\rcpi which shows consumer price index - we can use this to adjust our money figures for inflation\n\rpre_1985_starts which lists the shows that started before 1985\n\r\rThe main dataset I’m interested in is grosses so let’s see what’s in there:\nglimpse(grosses_raw)\r## Rows: 47,524\r## Columns: 14\r## $ week_ending \u0026lt;date\u0026gt; 1985-06-09, 1985-06-09, 1985-06-09, 1985-06-0...\r## $ week_number \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\r## $ weekly_gross_overall \u0026lt;dbl\u0026gt; 3915937, 3915937, 3915937, 3915937, 3915937, 3...\r## $ show \u0026lt;chr\u0026gt; \u0026quot;42nd Street\u0026quot;, \u0026quot;A Chorus Line\u0026quot;, \u0026quot;Aren\u0026#39;t We All...\r## $ theatre \u0026lt;chr\u0026gt; \u0026quot;St. James Theatre\u0026quot;, \u0026quot;Sam S. Shubert Theatre\u0026quot;,...\r## $ weekly_gross \u0026lt;dbl\u0026gt; 282368, 222584, 249272, 95688, 61059, 255386, ...\r## $ potential_gross \u0026lt;lgl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...\r## $ avg_ticket_price \u0026lt;dbl\u0026gt; 30.42, 27.25, 33.75, 20.87, 20.78, 31.96, 28.3...\r## $ top_ticket_price \u0026lt;lgl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...\r## $ seats_sold \u0026lt;dbl\u0026gt; 9281, 8167, 7386, 4586, 2938, 7992, 10831, 567...\r## $ seats_in_theatre \u0026lt;dbl\u0026gt; 1655, 1472, 1088, 682, 684, 1018, 1336, 1368, ...\r## $ pct_capacity \u0026lt;dbl\u0026gt; 0.7010, 0.6935, 0.8486, 0.8405, 0.5369, 0.9813...\r## $ performances \u0026lt;dbl\u0026gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 0, 8...\r## $ previews \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0...\rThere’s a lot of great data in there - I want to create a few things including a patchwork that gives us a quick look into how Broadway shows have performed over the last 35 years, a dashboard (mainly because I’ve been wanting to try flexdashboard and tidymetrics!) and a quick race bar chart (though I don’t think this’ll be great.)\nCleaning the data\rFor me, the main columns of interest are week_ending, weekly_gross, weekly_gross_overall, avg_ticket_price, seats_sold, pct_capacity and we’ll have to use performances and previews to clean up the data. I also want to use the tidymetrics package to add in some additional date columns like quarters, years, and would like a rolling average (not sure how long yet). To do that we need to rename the column week_ending to date and then use cross_by_periods. Before adding in any additional columns I want to clean the data though so let’s:\n\rRename week_ending to date\n\rMake sure there are no 0’s in any of the numerical columns as this will affect our average\n\rAccount for inflation (I’m going to cut off the data at the end of 2019 so will use last week of 2019)\n\r\rgrosses \u0026lt;- grosses_raw %\u0026gt;%\rrename(date = week_ending) %\u0026gt;%\rmutate_at(vars(weekly_gross:pct_capacity),\r~ ifelse(performances + previews == 0 | . == 0, NA, .))\r# quick check to see how many NAs there are now\rstr(map(grosses, ~ sum(is.na(.))))\r## List of 14\r## $ date : int 0\r## $ week_number : int 0\r## $ weekly_gross_overall: int 0\r## $ show : int 0\r## $ theatre : int 0\r## $ weekly_gross : int 125\r## $ potential_gross : int 47524\r## $ avg_ticket_price : int 125\r## $ top_ticket_price : int 47524\r## $ seats_sold : int 125\r## $ seats_in_theatre : int 88\r## $ pct_capacity : int 133\r## $ performances : int 0\r## $ previews : int 0\rWith the easy bits done, we need to account for inflation. This requires a few steps: dividing all the CPIs by the picked date (December 2019), converting weekly dates in grosses to monthly so we can merge the two together, mutate all of the columns in grosses that use dollars to be updated with the inflation rate.\ncpi \u0026lt;- cpi_raw %\u0026gt;%\rmutate(dec_2019_dollars = cpi[year_month == \u0026quot;2019-12-01\u0026quot;] / cpi)\rgrosses_clean \u0026lt;- grosses %\u0026gt;%\rfilter(year(date) \u0026gt; 1985 \u0026amp; year(date) \u0026lt; 2020) %\u0026gt;%\rmutate(year_month = floor_date(date, unit = \u0026quot;month\u0026quot;)) %\u0026gt;% left_join(cpi, by = \u0026quot;year_month\u0026quot;) %\u0026gt;% mutate_at(\rvars(\rweekly_gross_overall,\rweekly_gross,\rpotential_gross,\ravg_ticket_price,\rtop_ticket_price\r),\r~ . * dec_2019_dollars\r) %\u0026gt;%\rselect(-potential_gross, -top_ticket_price, -c(year_month:dec_2019_dollars)) %\u0026gt;%\rmutate(year = year(date))\r\rExploring\rNow that we’ve done the initial data setup it’d be interesting to look at a few things. In particular I’d like to see:\n\rHow my favourite shows have performed over the years (Lion King \u0026amp; Phantom)\n\rHas Broadway become more popular? In terms of both number of plays and attendance\n\rWhat’s the price increase like (have adjusted for inflation above)\n\rHow’s the annual gross revenue?\n\r\rAs mentioned before there are four metrics that can help us assess the popularity of Broadway shows over the last 35 years. Starting with the top-left plot there’s been a clear year-on-year growth with a peak happening in 2018. A few reasons for the decrease in growth in 2019 include Bruce Springsteen’s concert closed in 2018 and shows like Anastasia brought in a lot less money (44m to 9m for Anastaria). We’d expect revenue to go up as number of performances goes up but the fill shows that’s not necessarily the case. 2008, for example, had a lot of performances but that growth wasn’t reflected in revenue.\nThe top-right chart, seats sold, shows the average has gone up considerably. Can see a clear shift beginning in 2015 which is when Hamilton opened and as both Hamilton and Book of Mormon have been consistently sold out since the start of their run, it’s pushing number of seats sold up. It’s also interesting that The Lion King, despite it’s long run, is still consistently selling out 96-97% of its capacity.\nThe bottom left chart looks at the range of average ticket prices for shows throughout the years and Hamilton’s high prices (as shown further down in the post) are pushing up the averages. Most of the outliers belong to Hamilton and Book of Mormon, both of which charge much higher than average prices (and have crazy top prices!). The bottom right is a simple plot showing the number of unique shows has risen each year, showing there’s been an expansion throughout the years to accommodate all of these.\nJust to emphasise that more performances generally mean more money.. but not always (as in 2008):\nggplot(performances_and_shows, aes(x = yearly_gross,\ry = number_of_performances)) +\rgeom_point() + labs(x = \u0026quot;# of shows\u0026quot;,\ry = \u0026quot;yearly revenue\u0026quot;,\rtitle = \u0026quot;More performances, more money?\u0026quot;)\r\rSpecific shows\rAfter looking more into what made Broadway grow over time I found that the top10 shows were contributing a lot to the overall growth. This is especially true for Hamilton and Book of Mormon, two shows that have driven up the average ticket price for shows and have also set a gold standard for selling out consistently! I wanted to create a quick dashboard with flexdashboard so the graphs below will be reflected in that. You can find the dashboard here:\rlink to dashboard\nGoing to start with adding in the periods and rolling averages using tidymetrics to see if it’ll give us some good insights. As shown by David Robinson in a previous tidy tuesday (think it was on beer!) we can use both cross_by_dimensions which will add an ‘All’ to our data (in this case in our show column) and cross_by_periods where we can determine what periods to add into the data and what kind of window to summarise them by:\ntop_shows \u0026lt;- grosses_clean %\u0026gt;%\rgroup_by(show) %\u0026gt;%\rsummarise(total_gross = sum(weekly_gross, na.rm = TRUE)) %\u0026gt;%\rarrange(desc(total_gross)) %\u0026gt;%\rungroup() %\u0026gt;%\rhead(10) %\u0026gt;%\rselect(show) %\u0026gt;%\rpull()\rgrosses_summarised \u0026lt;- grosses_clean %\u0026gt;%\rfilter(show %in% top_shows) %\u0026gt;%\rcross_by_dimensions(show) %\u0026gt;%\rcross_by_periods(c(\u0026quot;month\u0026quot;, \u0026quot;quarter\u0026quot;, \u0026quot;year\u0026quot;),\rwindows = 28) %\u0026gt;% summarise(\rusd_gross = sum(weekly_gross, na.rm = TRUE),\ravg_ticket_price = mean(avg_ticket_price, na.rm = TRUE),\rnb_seats_sold = sum(seats_sold, na.rm = TRUE),\rpct_capacity = mean(pct_capacity, na.rm = TRUE)\r) %\u0026gt;%\rungroup() %\u0026gt;%\rmutate(year = year(date))\rgrosses_summarised %\u0026gt;%\rfilter(period == \u0026quot;quarter\u0026quot;,\rshow != \u0026quot;All\u0026quot;) %\u0026gt;%\rmutate(show = fct_reorder(show, avg_ticket_price)) %\u0026gt;%\rggplot(aes(x = avg_ticket_price,\ry = show,\rfill = show)) +\rgeom_density_ridges(scale = 4, size = 0, alpha = 0.7) +\rscale_fill_igv() +\rguides(fill = \u0026quot;none\u0026quot;) +\rlabs(x = \u0026quot;Average ticket price\u0026quot;, y = \u0026quot;Show\u0026quot;,\rtitle = \u0026quot;Distribution of ticket prices for top 10 Broadway shows\u0026quot;) +\rtheme(\rplot.title = element_text(hjust = -1)\r)\rHamilton is by far the most expensive show on average which explains why it’s already in the top 10 highest grossing theatres despite only opening in 2015. I’m guessing it’s also going to have one of the highest attendance rates so we’ll have a look at that after visualising total gross (which I already calculated above!):\ngrosses_summarised %\u0026gt;%\rfilter(show != \u0026quot;All\u0026quot;,\rperiod == \u0026quot;year\u0026quot;) %\u0026gt;% mutate(show = fct_reorder(show, usd_gross)) %\u0026gt;% ggplot(aes(x = date,\ry = usd_gross,\rfill = show)) +\rgeom_col() +\rcoord_flip() +\rscale_fill_igv() +\rscale_x_date(date_breaks = \u0026quot;5 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) +\rscale_y_continuous(labels = scales::dollar_format()) +\rtheme(\rlegend.text = element_text(size = rel(0.8)),\rlegend.title = element_blank(),\rlegend.position = \u0026quot;bottom\u0026quot;\r) +\rlabs(x = NULL,\ry = NULL,\rtitle = \u0026quot;Total revenue by Broadway show\u0026quot;)\rIt’s very obvious here the astronominal amounts Hamilton is bringing in. It slowly pops up in its first year but then continuously grows until 2019 where it slightly starts dropping off. Now that we have a stacked bar plot it also becomes clear where certain shows dropped off like Cats, which closed in 2000 despite still pulling in lots of money. It also had a reboot in 2016/2017 where people flocked to see it! The Book of Mormon is another relatively new show which has been massively successful and just like Hamilton, has a very high price tag.\ngrosses_summarised %\u0026gt;%\rfilter(period == \u0026quot;quarter\u0026quot;,\rshow != \u0026quot;All\u0026quot;) %\u0026gt;%\rggplot(aes(x = date,\ry = pct_capacity,\rcolour = show)) +\rgeom_line(size = 1) +\rscale_y_continuous(labels = scales::percent_format()) +\rexpand_limits(y = 0) +\rscale_color_igv() +\rlabs(x = \u0026quot;\u0026quot;,\ry = \u0026quot;seats sold\u0026quot;,\rtitle = \u0026quot;Average % of sold seats per show\u0026quot;) +\rtheme(\rlegend.text = element_text(size = rel(0.8)),\rlegend.title = element_blank(),\rlegend.position = \u0026quot;bottom\u0026quot;\r)\rQuite hard to interpret in a static plot with 10 data points but three things stand out immediately: 1) Hamilton consistently has above 100% seat capacity, 2) Book of Mormon has the same, and has been selling out since it started its run, 3) The Lion King has a surprisingly varying attendance rate. That said, out of all the long running shows, it still manages to sell out most of the time.\nNow that we have our main graphs I’ll add them to flexdashboard and add a few score cards to show overall aggregations of the main metrics used. Lastly I wanted to add a quick bar chart race for top 10 shows throughout the years. I didn’t think it would turn out great (it didn’t) but the method might be interesting:\n\rBar chart race\rshow_yearly_gross \u0026lt;- grosses_clean %\u0026gt;%\rselect(date, show, weekly_gross) %\u0026gt;%\rmutate(year = year(date)) %\u0026gt;%\rgroup_by(year, show) %\u0026gt;%\rmutate(yearly_gross = sum(weekly_gross)) %\u0026gt;%\rungroup() %\u0026gt;%\rdistinct(year, show, yearly_gross)\ryearly_top10_shows_rev \u0026lt;- show_yearly_gross %\u0026gt;%\rgroup_by(year) %\u0026gt;%\rmutate(rank = rank(-yearly_gross),\rValue_rel = yearly_gross/yearly_gross[rank==1], Value_lbl = paste0(\u0026quot; \u0026quot;, round(yearly_gross/1e7))) %\u0026gt;%\rgroup_by(show) %\u0026gt;%\rfilter(rank \u0026lt;= 10) %\u0026gt;%\rungroup()\rWe now have all the data we need to set up a static plot for every single year:\nshows_static \u0026lt;- ggplot(yearly_top10_shows_rev,\raes(x = rank,\rgroup = show,\rfill = as.factor(show),\rcolour = as.factor(show))) +\rgeom_tile(aes(y = yearly_gross / 2,\rheight = yearly_gross,\rwidth = 0.9), alpha = 0.8, colour = NA) +\rgeom_text(aes(y = 0, label = paste(show, \u0026quot; \u0026quot;)),\rvjust = 0.2, hjust = 1) +\rcoord_flip(clip = \u0026quot;off\u0026quot;, expand = FALSE) +\rscale_y_continuous(labels = scales::comma) +\rscale_x_reverse() +\rguides(color = FALSE, fill = FALSE) +\rtheme(axis.line=element_blank(),\raxis.text.x=element_blank(),\raxis.text.y=element_blank(),\raxis.ticks=element_blank(),\raxis.title.x=element_blank(),\raxis.title.y=element_blank(),\rlegend.position=\u0026quot;none\u0026quot;,\rpanel.background=element_blank(),\rpanel.border=element_blank(),\rpanel.grid.major=element_blank(),\rpanel.grid.minor=element_blank(),\rpanel.grid.major.x = element_line( size=.1, color=\u0026quot;grey\u0026quot; ),\rpanel.grid.minor.x = element_line( size=.1, color=\u0026quot;grey\u0026quot; ),\rplot.title=element_text(size=25, hjust=0.5, face=\u0026quot;bold\u0026quot;, colour=\u0026quot;grey\u0026quot;, vjust=-1),\rplot.subtitle=element_text(size=18, hjust=0.5, face=\u0026quot;italic\u0026quot;, color=\u0026quot;grey\u0026quot;),\rplot.caption =element_text(size=8, hjust=0.5, face=\u0026quot;italic\u0026quot;, color=\u0026quot;grey\u0026quot;),\rplot.background=element_blank(),\rplot.margin = margin(2,2, 2, 4, \u0026quot;cm\u0026quot;))\rshows_static +\rtransition_time(year)\rlibrary(gifski)\ranim = shows_static + transition_states(year,\rtransition_length = 8, state_length = 68) +\rview_follow(fixed_x = TRUE) +\rlabs(title = \u0026quot;Revenue per year : {closest_state}\u0026quot;,\rsubtitle = \u0026quot;Top 10 Shows\u0026quot;)\ranimate(anim, 300, fps = 20, width = 1200, height = 1000, renderer = gifski_renderer(\u0026quot;yearly_top10_shows_br.gif\u0026quot;))\rConclusion\rI really liked this TidyTuesday dataset, there’s a lot of data to play with. My main takeaway from all this:\n\rI need to go see Hamilton and Book of Mormon - it’s incredible how they have both managed to ‘change the game’. The fact that their average seat price is so high and they are still constantly over capacity shows that the hype around these shows isn’t dying down\n\rThe Lion King and Wicked are both still doing very well. Wicked especially is impressive to me due to its very long run!\n\rTidymetrics is incredible - it makes it so much easier to quickly aggregate metrics\n\rIt’s really easy to set up a flexdashboard and I’ll definitely be using lots more of Patchwork in the future\n\r\rAll of the code can be found in the blog section on my GitHub\n\r\r"
            }
        
    
]